{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "putting_it_all_together.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week13/putting_it_all_together.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcdHO6HqntD"
      },
      "source": [
        "# Practical 4c - Putting It All Together\n",
        "\n",
        "In this final part, we will take the models that we have trained and use them to recognize an intent and the entities and building simple responses to that. \n",
        "\n",
        "Before starting, click on the Colab's Runtime > Manage Sessions menu. Click the \"TERMINATE OTHER SESSIONS\" button.  \n",
        "\n",
        "Then, run the following cells to download the models (after training them to our intent and token classification tasks) and install the necessary libraries. \n",
        "\n",
        "The reason we are doing this is because downloading the models that you have trained from Colab is VERY slow. So we've already saved a copy of our own trained models and uploaded it to a public server on Amazon for download.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSsEBxfeqnTc",
        "outputId": "093936a5-1601-444c-de44-82fa0aa08183"
      },
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/intent_model.zip\n",
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/token_model.zip\n",
        "!unzip intent_model.zip\n",
        "!unzip token_model.zip\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-18 15:32:36--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/intent_model.zip\n",
            "Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.128.131\n",
            "Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.128.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 246825147 (235M) [application/zip]\n",
            "Saving to: ‘intent_model.zip’\n",
            "\n",
            "intent_model.zip    100%[===================>] 235.39M  17.8MB/s    in 15s     \n",
            "\n",
            "2021-06-18 15:32:52 (15.5 MB/s) - ‘intent_model.zip’ saved [246825147/246825147]\n",
            "\n",
            "--2021-06-18 15:32:52--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/token_model.zip\n",
            "Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.128.175\n",
            "Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.128.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244631057 (233M) [application/zip]\n",
            "Saving to: ‘token_model.zip’\n",
            "\n",
            "token_model.zip     100%[===================>] 233.30M  17.4MB/s    in 15s     \n",
            "\n",
            "2021-06-18 15:33:08 (15.1 MB/s) - ‘token_model.zip’ saved [244631057/244631057]\n",
            "\n",
            "Archive:  intent_model.zip\n",
            "caution: filename not matched:  token_model.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9SzGqc5rid"
      },
      "source": [
        "Next, run the following to install a specific version of the HuggingFace Transformers library.\n",
        "\n",
        "Our model was trained against this version of the library, so it is advisable to use the same version for prediction / inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1ZHduZsD9FM",
        "outputId": "385fcc70-1736-4944-a282-6f6ab915dd05"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfHlpofrhsw"
      },
      "source": [
        "## Section 1 - Inferring Intent\n",
        "\n",
        "In this section, we declare the codes to infer intent based on a single line of input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7arnC-lxUx0",
        "outputId": "2bd96aee-34f4-4047-c6d3-40e211aa6af2"
      },
      "source": [
        "# Import the necessary libraries\n",
        "#\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    TFAutoModelForTokenClassification,\n",
        "    TFAutoModelForSequenceClassification\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Create the DistilBERT tokenizer\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "\n",
        "# Create a list of unique labels that we will recognize.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# Define a function to perform inference on a single input text.\n",
        "# \n",
        "def infer_intent(model, text):\n",
        "    # Passes the text into the tokenizer\n",
        "    #\n",
        "    input = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "    # Sends the result from the tokenizer into our classification model\n",
        "    #\n",
        "    output = model(input)\n",
        "    pred_label = np.argmax(tf.nn.softmax(output.logits, axis=-1))\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return sentence_labels[pred_label]\n",
        "\n",
        "\n",
        "# Load the saved model file\n",
        "#\n",
        "intent_model = TFAutoModelForSequenceClassification.from_pretrained('intent_model')\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at intent_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at intent_model and are newly initialized: ['dropout_79']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfalBIty8HE9"
      },
      "source": [
        "Run the following cell to test the codes that infers the intent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SGku6HIC7tus",
        "outputId": "7fe72dab-dd0f-4364-a6ed-af450e046e5b"
      },
      "source": [
        "infer_intent(intent_model, \"How much is the ticket to fly to New York\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atis_airfare'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPh__bBSrsXg"
      },
      "source": [
        "## Section 2 - Inferring Entity\n",
        "\n",
        "In this section, we declare the codes to infer entities for each individual word in a line of text. The entities are then constructed and returned to the caller.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRFGoqUEvayO"
      },
      "source": [
        "# Define a list of unique labels that we will recognized\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Define the function to infer the individual tokens\n",
        "#\n",
        "\n",
        "def infer_tokens(model, text):\n",
        "    encodings = tokenizer([text], \n",
        "                          is_split_into_words=True, \n",
        "                          padding=True, \n",
        "                          truncation=True, \n",
        "                          return_offsets_mapping=True, \n",
        "                          return_tensors=\"tf\")\n",
        "\n",
        "    label_mapping = [0] * len(encodings.offset_mapping[0])\n",
        "    for i, offset in enumerate(encodings.offset_mapping[0]):\n",
        "        if encodings.offset_mapping[0][i][0] == 0 and encodings.offset_mapping[0][i][1] != 0:\n",
        "            label_mapping[i] = 1\n",
        "\n",
        "    encodings.pop(\"offset_mapping\")\n",
        "   \n",
        "    output = token_model(encodings)[0]\n",
        "\n",
        "    cur_index = -1\n",
        "    result_tokens = []\n",
        "    result_texts = []\n",
        "\n",
        "    for i in range(output.shape[1]):\n",
        "        if label_mapping[i] == 1:\n",
        "            result_tokens.append(np.argmax(output[0][i]).item())\n",
        "            result_texts.append(text[i])\n",
        "            cur_index += 1\n",
        "        else:\n",
        "            if cur_index >= 0 and text[i] != \"[CLS]\" and text[i] != \"[SEP]\":\n",
        "                result_texts[cur_index] += text[i].replace(\"##\", \"\")\n",
        "\n",
        "\n",
        "    return result_tokens, result_texts\n",
        "\n",
        "# Define the function to combine individual tokens into a dictionary\n",
        "#\n",
        "def infer_combined_tokens(token_model, text):\n",
        "    result = {\n",
        "        \"PER\" : [],\n",
        "        \"LOC\" : [],\n",
        "        \"ORG\" : [],\n",
        "        \"MISC\" : []\n",
        "    }\n",
        "\n",
        "    result_tokens, result_texts = infer_tokens(token_model, text)\n",
        "\n",
        "    current_token_label = \"\"\n",
        "    current_result_index = -1;    \n",
        "\n",
        "    for i in range(len(result_tokens)):\n",
        "        if token_labels[result_tokens[i]].startswith(\"B-\"):\n",
        "            current_token_label = token_labels[result_tokens[i]].replace(\"B-\", \"\")\n",
        "            result[current_token_label].append(result_texts[i])\n",
        "            current_result_index = len(result[current_token_label]) - 1\n",
        "        elif token_labels[result_tokens[i]].startswith(\"I-\"):\n",
        "            result[current_token_label][current_result_index] += \" \" + result_texts[i]\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Load the saved model file\n",
        "#\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFlcl1NOzqcQ",
        "outputId": "89368a8b-6fc7-47a0-c515-f5568bd5a6a4"
      },
      "source": [
        "token_model = TFAutoModelForTokenClassification.from_pretrained('token_model')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at token_model were not used when initializing TFDistilBertForTokenClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at token_model and are newly initialized: ['dropout_159']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnIpnGJ8AYv"
      },
      "source": [
        "Run the following cell to test the codes that extracts and combines all the entities for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK-8pQV574uI",
        "outputId": "5e480600-fbed-40a6-d00f-8778ffae7267"
      },
      "source": [
        "infer_combined_tokens(token_model, \"How much is the ticket to fly to New York\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LOC': [], 'MISC': [], 'ORG': [], 'PER': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fBzlRb7vbXe"
      },
      "source": [
        "## Section 3 - Implementing Logic for Our Chatbot\n",
        "\n",
        "In this section, let's implement some very basic logic for our chatbot. We will make use of the two functions that we wrote above.\n",
        "\n",
        "You can implement some simple logic that looks like the following:\n",
        "\n",
        "```\n",
        "        if (intent == \"atis_flight\" or intent == \"atis_airline\") and len(tokens[\"LOC\"]):\n",
        "            print (\"Can I confirmed if you just asked about flying to \" + tokens[\"LOC\"][0])\n",
        "        elif intent == \"atis_yes\":\n",
        "            print (\"Great, then let's me book the ticket for you\")\n",
        "        elif intent == \"atis_no\":\n",
        "            print (\"Oh I am sorry what did I get wrong?\")\n",
        "        elif intent == \"atis_greeting\":\n",
        "            print (\"Hi, how are you?\")            \n",
        "        else:\n",
        "            print (\"I don't quite know how to respond to \" + intent + \" yet.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBZ-U3dQLj3H"
      },
      "source": [
        "def chatbot():\n",
        "    print (\"Chatbot Started. Press 'Q'+Enter to quit.\")\n",
        "\n",
        "    while (True):\n",
        "        input_text = input()\n",
        "        if input_text == \"Q\" or input_text == \"\":\n",
        "            break\n",
        "\n",
        "        intent = infer_intent(input_text)\n",
        "        tokens = infer_combined_tokens(input_text)\n",
        "\n",
        "        # TODO: \n",
        "        # Write you own logic to conduct a conversation with the user\n",
        "        # about buying tickets and flying somewhere.\n",
        "        #...#\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    print (\"Good bye!\")\n",
        "\n",
        "chatbot()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}