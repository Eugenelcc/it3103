{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practical_4b_colab_token_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week13/4b_token_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXdc1O6Gm-a"
      },
      "source": [
        "# Practical 4b - Token Classification\n",
        "\n",
        "In this practical we will learn how to use the HuggingFace Transformers library to perform token classification.\n",
        "\n",
        "Just like what we did in Practical 3a, we will use the DistiBERT transformer architecture, which also allows us to classify each and every word in a sentence.\n",
        "\n",
        "####**NOTE: Be sure to set your runtime to a GPU instance!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy827DuG3b7"
      },
      "source": [
        "## Section 1 - Install Transformers\n",
        "\n",
        "Run the following cell to install the HuggingFace Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-D3yf5P0rWz",
        "outputId": "ff29364a-1d99-46b5-fef4-c144fd46c7f9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8paYUgs38H5"
      },
      "source": [
        "## Section 2 - Import, Define Classes and Helper Functions\n",
        "\n",
        "Run the following cell to import all necessary libraries, define the necessary variables, classes and functions required for our processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT5Wu4sFYorz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Initialize the DistilBERT tokenizer.\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define a function that loads up a space-, comma-, tab-separated file\n",
        "# and extracts the input word and label for each word.\n",
        "# \n",
        "# It is assumed that the file is in the CONLL format:\n",
        "#\n",
        "#      sentence1-word1, ..., label1-1\n",
        "#      sentence1-word2, ..., label1-2\n",
        "#      sentence1-word3, ..., label1-3\n",
        "#      <empty line>\n",
        "#      sentence2-word1, ..., label2-1\n",
        "#      sentence2-word2, ..., label2-2\n",
        "#      ...\n",
        "#      sentence2-wordn, ..., label2-n\n",
        "#      <empty line>\n",
        "#      ...\n",
        "#\n",
        "# This function returns a 2D list of words and a 2D list of labels\n",
        "# corresponding to each word.\n",
        "#\n",
        "def load_conll(filepath, delimiter=' ', word_column_index=0, label_column_index=3):\n",
        "    all_texts = []\n",
        "    all_tags = []\n",
        "\n",
        "    texts = []\n",
        "    tags = []\n",
        "\n",
        "    # Opens the file.\n",
        "    #\n",
        "    with open(filepath, \"r\") as f:\n",
        "\n",
        "        # Loops through each line \n",
        "        for line in f:\n",
        "\n",
        "            # Split each line by its delimiter (default is a space)\n",
        "            tokens = line.split(delimiter)\n",
        "\n",
        "            # If the line is empty, treat it as the end of the\n",
        "            # previous sentence, and construct a new sentence\n",
        "            #\n",
        "            if len(tokens) == 1:\n",
        "                # Append the sentence\n",
        "                # \n",
        "                all_texts.append(texts)\n",
        "                all_tags.append(tags)\n",
        "\n",
        "                # Create a new sentence\n",
        "                #\n",
        "                texts = []\n",
        "                tags = []\n",
        "            else:\n",
        "                # Not yet end of the sentence, continue to add\n",
        "                # words into the current sentence\n",
        "                #\n",
        "                thistext = tokens[word_column_index].replace('\\n', '')\n",
        "                thistag = tokens[label_column_index].replace('\\n', '')\n",
        "\n",
        "                texts.append(thistext)\n",
        "                tags.append(thistag)\n",
        "\n",
        "    # Insert the last sentence if it contains at least 1 word.\n",
        "    #\n",
        "    if len(texts) > 0:\n",
        "        all_texts.append(texts)\n",
        "        all_tags.append(tags)\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return all_texts, all_tags\n",
        "\n",
        "\n",
        "# This function is taken from HuggingFace's documentation at:\n",
        "# https://huggingface.co/transformers/custom_datasets.html\n",
        "#\n",
        "# This function simply converts the string classification tags for each\n",
        "# word into their index using the token_labels_id_by_label dictionary.\n",
        "#\n",
        "# Also, it uses the offset_mapping to determine which words are [CLS],\n",
        "# [SEP] and sub-words so that we can leave the tag as a -100 value \n",
        "# (ignored).\n",
        "# \n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[token_labels_id_by_label[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDg6sNNC4Dq5"
      },
      "source": [
        "## Section 3 - Defining Our Classification Labels\n",
        "\n",
        "Run the following cell to declare the token classification labels that we will be using.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOghzNYqE5v"
      },
      "source": [
        "\n",
        "# Define a list of unique token labels that we will recognize\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Create a reverse-mapping dictionary of the label -> index.\n",
        "#\n",
        "token_labels_id_by_label = {tag: id for id, tag in enumerate(token_labels)}\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWnSGGfUj91H",
        "outputId": "af2310dc-a665-4043-8d1a-33b060987909"
      },
      "source": [
        "token_labels_id_by_label"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC': 5,\n",
              " 'B-MISC': 7,\n",
              " 'B-ORG': 3,\n",
              " 'B-PER': 1,\n",
              " 'I-LOC': 6,\n",
              " 'I-MISC': 8,\n",
              " 'I-ORG': 4,\n",
              " 'I-PER': 2,\n",
              " 'O': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SN5j-9-4n8I"
      },
      "source": [
        "## Section 4 - Load and Split Our Data\n",
        "\n",
        "We are now prepared to process our data. \n",
        "\n",
        "Go ahead and upload the token_train.txt, token_test.txt file into Colab.\n",
        "\n",
        "Then, fill up the codes below to load the data from the token_train.txt, token_test.txt file.\n",
        "   ```\n",
        "   train_texts, train_tags = load_conll(\"token_train.txt\")\n",
        "   val_texts, val_tags = load_conll(\"token_test.txt\")\n",
        "   ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVS5TewcrW0",
        "outputId": "bc36b608-3712-4333-f87a-9fdf26101aa6"
      },
      "source": [
        "load_conll('try.txt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['-DOCSTART-'],\n",
              "  ['SOCCER',\n",
              "   '-',\n",
              "   'JAPAN',\n",
              "   'GET',\n",
              "   'LUCKY',\n",
              "   'WIN',\n",
              "   ',',\n",
              "   'CHINA',\n",
              "   'IN',\n",
              "   'SURPRISE',\n",
              "   'DEFEAT',\n",
              "   '.'],\n",
              "  ['Nadim', 'Ladki'],\n",
              "  ['AL-AIN', ',']],\n",
              " [['O'],\n",
              "  ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'],\n",
              "  ['B-PER', 'I-PER'],\n",
              "  ['B-LOC', 'O']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpUOQu9kqGiP",
        "outputId": "97fb4682-81b3-4103-8e2a-1b8d93842602"
      },
      "source": [
        "# TODO:\n",
        "# Loads the training and test text files.\n",
        "#...#\n",
        "train_texts, train_tags = load_conll(\"token_train.txt\")\n",
        "val_texts, val_tags = load_conll(\"token_test.txt\")\n",
        "\n",
        "\n",
        "\n",
        "print (train_texts[0:5])\n",
        "print (train_tags[0:5])\n",
        "print (len(train_texts))\n",
        "print (len(val_texts))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-DOCSTART-'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.']]\n",
            "[['O'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O'], ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
            "14987\n",
            "3684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJS45b6_WI8v",
        "outputId": "9171b85e-8875-496d-9d14-655b986e3260"
      },
      "source": [
        "len(train_texts[4])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbSW-APAWWaq",
        "outputId": "5403acba-e37c-40a3-a1a5-4441f3e68785"
      },
      "source": [
        "len(train_tags[4])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAonFBXU5EWX"
      },
      "source": [
        "## Section 5 - Preparing Our Data for Training\n",
        "\n",
        "Modify the following cell to:\n",
        "\n",
        "1. Tokenize all the training and validation input texts into individual word indexes and attention masks.\n",
        "   ```\n",
        "   train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "   ```\n",
        "\n",
        "2. Convert the individual word tags into their corresponding indexes. \n",
        "   ```\n",
        "   train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)\n",
        "   ```\n",
        "\n",
        "3. Remove the 'offset_mapping' since we do not require that for training.\n",
        "   ```\n",
        "   train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "   ```\n",
        "\n",
        "3. Construct the TokenClassificationDataset in preparation for training.\n",
        "   ```\n",
        "   train_dataset = TokenClassificationDataset(train_encodings, train_labels)\n",
        "val_dataset = TokenClassificationDataset(val_encodings, val_labels)\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-hLeAus5DgZ"
      },
      "source": [
        "# TODO:\n",
        "# Call the tokenizer to assign word indexes to each word.\n",
        "#\n",
        "# NOTE: When loading up the data from the train.txt and test.txt (CONLL format),\n",
        "# the words have already been split up. \n",
        "#...#\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)\n",
        "\n",
        "# TODO:\n",
        "# Call the encode_tags function to convert the string-based tag per word\n",
        "# into numeric indexes.\n",
        "#...#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO:\n",
        "# Remove the offset_mapping list as we don't need it for training.\n",
        "#...#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO:\n",
        "# Construct the data set to be used for training.\n",
        "#...#\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjxZLsUuicNj"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxisdqSj36C-"
      },
      "source": [
        "Run the following cell below to see the train_texts and individual samples in the dataset for the first few lines of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu7FKTq7mEbo",
        "outputId": "de28cd90-688c-4370-d446-1d2d13efd251"
      },
      "source": [
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "\n",
        "for i in range(10):\n",
        "    print(train_texts[i])\n",
        "    print(list(train_dataset.take(1).as_numpy_iterator()))\n",
        "    print(\"---\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14987\n",
            "3684\n",
            "['-DOCSTART-']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['Peter', 'Blackburn']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['BRUSSELS', '1996-08-22']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n",
            "['Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.']\n",
            "[({'input_ids': array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}, array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32))]\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDGUmBIPfp4"
      },
      "source": [
        "## Section 6 - Train our Token Classification Model\n",
        "\n",
        "Run the following cell below to train the token classification model.\n",
        "\n",
        "Now, this training per epoch will take up a while complete. If it takes too long, ensure that you updated your runtime to use a GPU instance. If it still takes too long, we'll leave it running for 5 minutes, and use a saved model that I've already trained with this same dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOcCiXN-06lx"
      },
      "source": [
        "from transformers import DistilBertForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "token_model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(token_labels))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=token_model,                   # the instantiated Token Classification 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Vqmv0VAfBB"
      },
      "source": [
        "## Section 7 - Save our Token Classification Model\n",
        "\n",
        "Once your training is complete, save the model and download it to your own computer before the session expires!\n",
        "\n",
        "Alternatively, you can connect to and push your model to Google Drive once your training has completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkYk0B5NKAGP"
      },
      "source": [
        "torch.save(token_model, 'tokenclassification.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9pUdf68qHn"
      },
      "source": [
        "## Section 8 - Evaluate the Model\n",
        "\n",
        "Run the following cells below to evaluate your model performance.\n",
        "\n",
        "Obviously, you can only do this AFTER your training is completed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqobMwQOGiYP"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "# Initialize the DistilBERT tokenizer.\n",
        "#\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define a list of unique labels that we will recognized\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Define the function to infer the individual tokens\n",
        "#\n",
        "def infer_tokens(text):\n",
        "    encodings = tokenizer([text], is_split_into_words=True, padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\")\n",
        "\n",
        "    label_mapping = [0] * len(encodings.offset_mapping[0])\n",
        "    for i, offset in enumerate(encodings.offset_mapping[0]):\n",
        "        if encodings.offset_mapping[0][i][0] == 0 and encodings.offset_mapping[0][i][1] != 0:\n",
        "            label_mapping[i] = 1\n",
        "\n",
        "    encodings.pop(\"offset_mapping\")\n",
        "    encodings = encodings.to(\"cuda\")\n",
        "\n",
        "    # Use the token classification model to predict the labels\n",
        "    # for each word.\n",
        "    #\n",
        "    output = token_model.forward(**encodings)[0].detach().to(\"cpu\")\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for i in range(output.shape[1]):\n",
        "        if label_mapping[i] == 1:\n",
        "            result.append(np.argmax(output[0][i]).item())\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmI5r-tY-4y0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# This function takes in a list of sentences (texts) and passes them into the\n",
        "# infer_tokens method to tokenize and predict each word's label.\n",
        "# \n",
        "# It will then convert the list of labels into their numeric index, and\n",
        "# return both actual label and predicted label to the caller.\n",
        "#\n",
        "def get_actual_pred_y(texts, labels):\n",
        "    all_actual_y = []\n",
        "    all_pred_y = []\n",
        "\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        x = texts[i]\n",
        "\n",
        "        actual_y = list(filter(lambda x: x != -100, labels[i]))\n",
        "        pred_y = infer_tokens(x)\n",
        "\n",
        "        if (len(actual_y) == len(pred_y)):\n",
        "            all_actual_y += actual_y\n",
        "            all_pred_y += pred_y\n",
        "        else:\n",
        "            print (\"Error: %d, %d, %d, %s \" % (i, len(actual_y), len(pred_y), x ))\n",
        "\n",
        "    return all_actual_y, all_pred_y\n",
        "\n",
        "# Get the actual and predicted labels for all words in all sentences\n",
        "# for both the training and the test set.\n",
        "# \n",
        "actual_y_train, pred_y_train = get_actual_pred_y(train_texts, train_labels)\n",
        "actual_y_test, pred_y_test = get_actual_pred_y(val_texts, val_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5jUinaajcsh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def display_model_evaluation_results(y_train, pred_y_train, y_test, pred_y_test, labels):\n",
        "    \n",
        "    plt.figure(figsize=(20,6))  \n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Print the first Confusion Matrix for the training data\n",
        "    #\n",
        "    cm = confusion_matrix(y_train, pred_y_train)\n",
        "    print (cm.shape)\n",
        "\n",
        "    cm_df = pd.DataFrame(cm, labels, labels)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Confusion Matrix (Train Data)')\n",
        "    sns.heatmap(cm_df, annot=True)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')        \n",
        "    \n",
        "    # Print the second Confusion Matrix for the test data\n",
        "    #    \n",
        "    cm = confusion_matrix(y_test, pred_y_test)\n",
        "    \n",
        "    cm_df = pd.DataFrame(cm, labels, labels)          \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Confusion Matrix (Test Data)')\n",
        "    sns.heatmap(cm_df, annot=True)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')        \n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    # Finally display the classification reports\n",
        "    #\n",
        "    print (\"Train Data\")\n",
        "    print (\"--------------------------------------------------------\")\n",
        "    print(classification_report(y_train, pred_y_train, target_names=labels))\n",
        "    print (\"\")\n",
        "    print (\"Test Data\")\n",
        "    print (\"--------------------------------------------------------\")\n",
        "    print(classification_report(y_test, pred_y_test, target_names=labels))\n",
        "\n",
        "\n",
        "display_model_evaluation_results(actual_y_train, pred_y_train, actual_y_test, pred_y_test, token_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}