{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf2env)",
      "language": "python",
      "name": "tf2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "bert-embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week12/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKanFNbpTfv4"
      },
      "source": [
        "# Using BERT as Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8phFkS4Tfv7"
      },
      "source": [
        "Other than fine-tuning BERT for downstream task such as text classification, we can use pretrained BERT model as a feature extractor, very much the same as we are using pretrained CNN such as ResNet as feature extractors for downstream task such as image classification and object detection.  \n",
        "\n",
        "In this lab, we will see how we use a pretrained DistilBert Model to extract features (or embedding) from text and use the extracted features (embeddings) to train a classifier to classify text. You can contrast this with the other lab where we train the DistilBert end to end for the classification, and compare the performance of both. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- extract text embeddings from the bert model \n",
        "- use the extracted features for text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nte0QWOZwOAw"
      },
      "source": [
        "## Install Hugging Face Transformers library\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcAO5A0oVMOj",
        "outputId": "40693f0f-c8d7-45b8-ea63-f5dcc675bfd1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiiqcrhLTfv8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os \n",
        "import shutil\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModel,\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh7XepZEZ3Ll",
        "outputId": "68a0a0ee-caf7-45da-b1d0-84505ee3083b"
      },
      "source": [
        "# downloaded the datasets.\n",
        "\n",
        "!wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
        "!wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-16 12:22:21--  https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
            "Resolving nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)... 52.219.132.175\n",
            "Connecting to nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)|52.219.132.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13308773 (13M) [text/csv]\n",
            "Saving to: ‘imdb_test.csv’\n",
            "\n",
            "imdb_test.csv       100%[===================>]  12.69M  4.57MB/s    in 2.8s    \n",
            "\n",
            "2021-06-16 12:22:25 (4.57 MB/s) - ‘imdb_test.csv’ saved [13308773/13308773]\n",
            "\n",
            "--2021-06-16 12:22:25--  https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv\n",
            "Resolving nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)... 52.219.128.83\n",
            "Connecting to nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)|52.219.128.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52953551 (50M) [text/csv]\n",
            "Saving to: ‘imdb_train.csv’\n",
            "\n",
            "imdb_train.csv      100%[===================>]  50.50M  10.6MB/s    in 6.2s    \n",
            "\n",
            "2021-06-16 12:22:32 (8.11 MB/s) - ‘imdb_train.csv’ saved [52953551/52953551]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we7dCLYWZ45L"
      },
      "source": [
        "train_df = pd.read_csv('imdb_train.csv')\n",
        "test_df = pd.read_csv('imdb_test.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD78Yn9zw5Yw"
      },
      "source": [
        "The train set has 40000 samples. We will a small subset (e.g. 2000) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model. We use dataframe's sample() to randomly select a subset of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGpIN8KExCOH"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE)\n",
        "test_df = test_df.sample(n=TEST_SIZE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcdskhQxIcH"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CQXjq-yxJku",
        "outputId": "6423e792-4ba6-452a-e50c-814f55aff070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df.sentiment.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1004\n",
              "1     996\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ORi78_VxN3f"
      },
      "source": [
        "train_texts = train_df['review']\n",
        "train_labels = train_df['sentiment']\n",
        "test_texts = test_df['review']\n",
        "test_labels = test_df['sentiment']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ikfIcafxSMC"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboqVJWTfwA"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  This is the same as the other lab exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5THnkPITfwA"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ltReujTfwA"
      },
      "source": [
        "The pretrained DistilBERT [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) expects a string or list of string, so we need to convert the data frame (or series) into list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFbiO3STfwB"
      },
      "source": [
        "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnqjMMCWTfwB"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgdY5C9TfwB"
      },
      "source": [
        "We will create a tensorflow dataset and use it's efficient batching later to obtain the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Znx8EnTfwB"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HNwF7yYTfwB"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    train_encodings['input_ids'],\n",
        "    train_labels\n",
        ")).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    test_encodings['input_ids'],\n",
        "    test_labels\n",
        ")).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU6uUMnATfwC"
      },
      "source": [
        "train_data = train_dataset.as_numpy_iterator()\n",
        "test_data = test_dataset.as_numpy_iterator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awI02BNyhRvO"
      },
      "source": [
        "len(train_encodings['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBuGjl7fTfwC"
      },
      "source": [
        "Here we instantiate a pretrained model from 'distilbert-base-cased' and specify output_hidden_state=True so that we get the output from each of the attention layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Pgl3CDTfwC"
      },
      "source": [
        "## Feature Extraction using (Distil)BERT. \n",
        "\n",
        "Here we will load the pretrained model for distibert-based-uncased and use it to extract features from the text (i.e. emeddings). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKSzCCRyTfwC"
      },
      "source": [
        "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\",output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrWYnDBTfwD"
      },
      "source": [
        "The model will produce two outputs: the 1st output `output[0]` is of shape `(16, 512, 768)` which corresponds to the output of the last hidden layer and the second output `output[1]` is a list of 7 outputs of shape `(16, 512, 768)`, corresponding to the output of each of the 7 attention layers. 768 refers to the hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B793BJtwTfwD"
      },
      "source": [
        "train_embeddings = None\n",
        "\n",
        "for batch in train_data:\n",
        "    output = model.predict(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if train_embeddings is None:\n",
        "        train_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        train_embeddings = np.vstack([train_embeddings, sentence_embeddings])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5i4L7XTfwD"
      },
      "source": [
        "test_embeddings = None\n",
        "\n",
        "for batch in test_data:\n",
        "    output = model.predict(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if test_embeddings is None:\n",
        "        test_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        test_embeddings = np.vstack([test_embeddings, sentence_embeddings])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhAG0WnTfwD"
      },
      "source": [
        "## Train a classifier using the extracted features (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zOWEm85TfwE"
      },
      "source": [
        "X_train = train_embeddings\n",
        "y_train = train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fzAPLL2TfwE"
      },
      "source": [
        "X_test = test_embeddings\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiA0JMdzTfwE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhBq-1sTfwE"
      },
      "source": [
        "clf = LinearSVC()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyaH5WnxTfwE"
      },
      "source": [
        "We should be getting an accuracy score of around 80% which is quite good, considering we are training with only 2000 samples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVDxRUdTfwE"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "1. Modify the code to use the output from a different attention layer as input features (embeddings) to the classifier. \n",
        "2. Try to generate sentence embeddings using different strategy, e.g. take a average of the output from multiple layers instead of only a single layer.\n",
        "2. Modify the code to use BERT model and see if it performs better than the DistilBERT. For BERT Model, the output of different layers are in `output[2]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUesPy8ATfwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}