{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf2env)",
      "language": "python",
      "name": "tf2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "bert-finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week12/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaeZuPBfwya"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/master/session-8/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Fine-tuning BERT for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrE6epjAfwyd"
      },
      "source": [
        "One of the approach where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model. \n",
        "\n",
        "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- configure the transformer model for fine-tuning \n",
        "- train the model for binary and multi-class text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVZlas4f4NK",
        "outputId": "56b6eeff-9948-4e74-a6ab-ea342377c300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 30.3MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 34.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 32.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 33.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 34.8MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 30.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 31.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 92kB 32.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 30.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 112kB 30.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 122kB 30.4MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 30.4MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 30.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 153kB 30.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 163kB 30.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 174kB 30.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 184kB 30.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 194kB 30.4MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 30.4MB/s eta 0:00:01\r\u001b[K     |███                             | 215kB 30.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 225kB 30.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 235kB 30.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 245kB 30.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 256kB 30.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 266kB 30.4MB/s eta 0:00:01\r\u001b[K     |████                            | 276kB 30.4MB/s eta 0:00:01\r\u001b[K     |████                            | 286kB 30.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 296kB 30.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 307kB 30.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 317kB 30.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 327kB 30.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 337kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 348kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 358kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 368kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 378kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 389kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 399kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 409kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 419kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 430kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 440kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 450kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 460kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 471kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 481kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 491kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 501kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 512kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 522kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 532kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 542kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 552kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 563kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 573kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 583kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 593kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 604kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 614kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 624kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 634kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 645kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 655kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 665kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 675kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 686kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 696kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 706kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 716kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 727kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 737kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 747kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 757kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 768kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 778kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 788kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 798kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 808kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 819kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 829kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 839kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 849kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 860kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 870kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 880kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 890kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 901kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 911kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 921kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 931kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 942kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 952kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 962kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 972kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 983kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 993kB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.6MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.7MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.8MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.9MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.0MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.1MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.2MB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 44.5MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVC0VH5Rfwyd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments,\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfUyhxsfwye"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di5Bf6u2fwye",
        "outputId": "8a32e915-9fad-4aad-e86b-c538f4766ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Uncomment the following if you have not downloaded the datasets.\n",
        "\n",
        "!wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
        "!wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-16 06:50:35--  https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
            "Resolving nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)... 52.219.32.143\n",
            "Connecting to nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)|52.219.32.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13308773 (13M) [text/csv]\n",
            "Saving to: ‘imdb_test.csv.1’\n",
            "\n",
            "imdb_test.csv.1     100%[===================>]  12.69M  4.87MB/s    in 2.6s    \n",
            "\n",
            "2021-06-16 06:50:39 (4.87 MB/s) - ‘imdb_test.csv.1’ saved [13308773/13308773]\n",
            "\n",
            "--2021-06-16 06:50:39--  https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv\n",
            "Resolving nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)... 52.219.36.31\n",
            "Connecting to nyp-aicourse.s3-ap-southeast-1.amazonaws.com (nyp-aicourse.s3-ap-southeast-1.amazonaws.com)|52.219.36.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52953551 (50M) [text/csv]\n",
            "Saving to: ‘imdb_train.csv.1’\n",
            "\n",
            "imdb_train.csv.1    100%[===================>]  50.50M  11.8MB/s    in 5.3s    \n",
            "\n",
            "2021-06-16 06:50:45 (9.52 MB/s) - ‘imdb_train.csv.1’ saved [52953551/52953551]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNjl2iy8fwyf"
      },
      "source": [
        "train_df = pd.read_csv('imdb_train.csv')\n",
        "test_df = pd.read_csv('imdb_test.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeFtqmAEfwyf"
      },
      "source": [
        "TRAIN_SIZE = 2500\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE)\n",
        "test_df = test_df.sample(n=TEST_SIZE)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwiiebLfwyf"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d1tQUKNfwyg",
        "outputId": "85bd5378-314b-4c1a-9d40-04416daf1137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df.sentiment.value_counts()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1266\n",
              "1    1234\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzBCEG1efwyg"
      },
      "source": [
        "train_texts = train_df['review']\n",
        "train_labels = train_df['sentiment']\n",
        "test_texts = test_df['review']\n",
        "test_labels = test_df['sentiment']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQnWEA2fwyh"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM49b__4fwyh"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  The tokenizer helps to produce the input tokens that are suitable to be used by the model, e.g. it automatically append the \\[CLS\\] token in the front of the sentence and the \\[SEP\\] token at the end of the token, and also the attention mask for those padded positions in the input sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd492GPqfwyh",
        "outputId": "eccb8543-2c5e-4ee6-d68d-2ef9d581e219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:517] 2021-06-16 06:50:46,073 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:553] 2021-06-16 06:50:46,079 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:50:46,213 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:50:46,214 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:50:46,216 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:50:46,218 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:50:46,220 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-wLa1yfwyh"
      },
      "source": [
        "The DistilBERT tokenizer (identical to Bert tokenizer) use WordPiece vocabulary. It has close to 30000 words and it maps pretrained embeddings for each. Each word has its own ids, we would need to map the tokens to those ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFAN7yoZfwyi",
        "outputId": "b2a780b6-6879-4518-f23f-b3385267a3f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Tokenizer vocab size = {tokenizer.vocab_size}\")\n",
        "print(list(tokenizer.vocab.keys())[6000:6020])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer vocab size = 30522\n",
            "['expectations', 'candace', 'fc', 'stitches', 'walks', '##β', 'affection', 'week', 'helmut', '##laid', '##lean', '##rard', 'ᆸ', '##shi', '##ica', 'continental', 'lovely', 'smith', 'orders', 'opponent']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOs5f0Idfwyi"
      },
      "source": [
        "Let us take a closer look at the output of the tokenization process. \n",
        "\n",
        "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padding. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
        "\n",
        "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. Similarly, 'Processing' is tokenized as 'Process' and '##ing'.  The '##' means that the rest of the token should be attached to the previous one.\n",
        "\n",
        "We also see that the tokenizer appended \\[CLS\\] to the beginning of the token sequence, and \\[SEP\\] at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS2hQw8ifwyi",
        "outputId": "5e975d0c-60e5-49e3-caf5-16ddfdf05978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
        "\n",
        "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
        "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
        "\n",
        "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
        "\n",
        "print(f\"tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'])}\")\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding keys:  dict_keys(['input_ids', 'attention_mask'])\n",
            "\n",
            "token ids: [101, 10938, 2121, 2003, 2428, 2204, 2005, 3019, 2653, 6364, 1012, 102]\n",
            "\n",
            "tokens: ['[CLS]', 'transform', '##er', 'is', 'really', 'good', 'for', 'natural', 'language', 'processing', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_npR61rfwyj"
      },
      "source": [
        "Now let's go ahead and tokenize our texts. But before we do so, we need to convert the pandas series to list first as the tokenizer cannot work with pandas series or dataframe directly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDiYuDTfwyj"
      },
      "source": [
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()\n",
        "test_texts = test_texts.to_list()\n",
        "test_labels = test_labels.to_list()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23BuebaNfwyj"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygaYFdjfwyk"
      },
      "source": [
        "We then create a tf dataset using the encodings and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJBnhuXfwyk"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWzPjzQfwyk"
      },
      "source": [
        "## Fine-tuning the model\n",
        "\n",
        "Now let us fine-tune our pre-trained model by training it with our custom dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTjliwfEfwyk"
      },
      "source": [
        "We first instantiate a DistilBert config object, and customise it to suit our needs. In our case, we will just specify the *num_labels* to tell the model how many labels to use in the last layer (classification layer). You only need to specify this if you are doing multi-class classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjt0S5Opfwyl",
        "outputId": "ff748a87-5436-481c-ab26-c7f26e8df96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", \n",
        "                                    num_labels=2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:517] 2021-06-16 06:51:02,413 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:553] 2021-06-16 06:51:02,416 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi4-umnlfwyl"
      },
      "source": [
        "We then instantiate a DistilBert model using this config object. If the config object is not passed, the default is a binary classification. The model is a a `tf.keras.Model` subclass. So you can train the model using Keras API such as `fit()`, or use Tensorflow custom training loops if you want to have more control over the training. The transformer library however, provides a Trainer class which abstract away the complex training loop, and supports distributed training on multi-GPU system. We will use this to train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCWbjeqfwyl"
      },
      "source": [
        "To use the Trainer class, we need to setup the training arguments such as number of epochs, batch sizes, warming up steps (commonly used in training Transformer model), weight decay (used to by Adam Optimizer for regularization purpose), learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg8QONG7fwyl",
        "outputId": "5dd08156-a4e1-4396-e5ec-e2add7ac8c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|training_args.py:710] 2021-06-16 06:51:03,190 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:616] 2021-06-16 06:51:03,193 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHuehIKfwym",
        "outputId": "7b6a8a30-f0d7-4e1f-b994-d975f63e22fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## for distributed training on multi-gpu system, uncomment the following \n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        config=config)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|training_args_tf.py:192] 2021-06-16 06:51:04,855 >> Tensorflow: setting up strategy\n",
            "[INFO|modeling_tf_utils.py:1261] 2021-06-16 06:51:04,967 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/fa107dc22c014df078a1b75235144a927f7e9764916222711f239b7ee6092ec9.bc4b731be56d8422e12b1d5bfa86fbd81d18d2770da1f5ac4f33640a17b7dde9.h5\n",
            "[WARNING|modeling_tf_utils.py:1311] 2021-06-16 06:51:05,484 >> Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_tf_utils.py:1323] 2021-06-16 06:51:05,485 >> Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V76-cZvfwym"
      },
      "source": [
        "We then define a function `compute_metrics()`  that will be used to compute metrics at evaluation. it takes in a EvalPrediction and return a dictionary string to metric values. In our case we just return the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz01grVWfwym"
      },
      "source": [
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"acc\": (preds == p.label_ids).mean()}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfOSV4zBfwym",
        "outputId": "61f08ca5-1765-4333-fafc-79232efa5079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We define a tensorboard writer \n",
        "writer = tf.summary.create_file_writer(\"tblogs\")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    tb_writer=writer\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:117] 2021-06-16 06:51:08,594 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "[INFO|trainer_tf.py:125] 2021-06-16 06:51:08,599 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci79f1pMfwym"
      },
      "source": [
        "We start the training, and do the evaluation. On a single-GPU system, the training will around 6-7 minutes to complete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7zJMRydfwyn",
        "outputId": "f804aa90-429e-49d0-d217-32aa1b8525f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainer.train()\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:528] 2021-06-16 06:51:12,791 >> ***** Running training *****\n",
            "[INFO|trainer_tf.py:529] 2021-06-16 06:51:12,792 >>   Num examples = 2000\n",
            "[INFO|trainer_tf.py:531] 2021-06-16 06:51:12,794 >>   Num Epochs = 1.0\n",
            "[INFO|trainer_tf.py:532] 2021-06-16 06:51:12,795 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer_tf.py:534] 2021-06-16 06:51:12,796 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer_tf.py:536] 2021-06-16 06:51:12,797 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer_tf.py:537] 2021-06-16 06:51:12,804 >>   Steps per epoch = 125\n",
            "[INFO|trainer_tf.py:538] 2021-06-16 06:51:12,805 >>   Total optimization steps = 125\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:404] 2021-06-16 06:51:27,354 >> {'loss': 0.68808204, 'learning_rate': 1e-06, 'epoch': 0.08, 'step': 10}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:51:35,859 >> {'loss': 0.6913229, 'learning_rate': 2e-06, 'epoch': 0.16, 'step': 20}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:51:44,398 >> {'loss': 0.6924981, 'learning_rate': 2.9999999e-06, 'epoch': 0.24, 'step': 30}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:51:52,978 >> {'loss': 0.69271827, 'learning_rate': 4e-06, 'epoch': 0.32, 'step': 40}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:01,629 >> {'loss': 0.6924953, 'learning_rate': 5.0000003e-06, 'epoch': 0.4, 'step': 50}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:10,309 >> {'loss': 0.69149435, 'learning_rate': 5.9999998e-06, 'epoch': 0.48, 'step': 60}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:19,022 >> {'loss': 0.69050235, 'learning_rate': 6.9999996e-06, 'epoch': 0.56, 'step': 70}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:27,795 >> {'loss': 0.68801606, 'learning_rate': 8e-06, 'epoch': 0.64, 'step': 80}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:36,593 >> {'loss': 0.6836868, 'learning_rate': 9e-06, 'epoch': 0.72, 'step': 90}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:45,421 >> {'loss': 0.6730271, 'learning_rate': 1.0000001e-05, 'epoch': 0.8, 'step': 100}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:52:54,254 >> {'loss': 0.65662867, 'learning_rate': 1.1e-05, 'epoch': 0.88, 'step': 110}\n",
            "[INFO|trainer_tf.py:404] 2021-06-16 06:53:03,140 >> {'loss': 0.6340283, 'learning_rate': 1.19999995e-05, 'epoch': 0.96, 'step': 120}\n",
            "[INFO|trainer_tf.py:610] 2021-06-16 06:53:07,590 >> Training took: 0:01:54.781678\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_L3bBZlfwyn",
        "outputId": "8e698ba3-70f7-4a07-d2bc-4aaf44982190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:306] 2021-06-16 06:53:07,609 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-06-16 06:53:07,613 >>   Num examples in dataset = 500\n",
            "[INFO|trainer_tf.py:309] 2021-06-16 06:53:07,614 >>   Num examples in used in evaluation = 512\n",
            "[INFO|trainer_tf.py:310] 2021-06-16 06:53:07,616 >>   Batch size = 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:404] 2021-06-16 06:53:18,216 >> {'eval_loss': 0.31664034724235535, 'eval_acc': 0.880859375, 'epoch': 1.0, 'step': 125}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_acc': 0.880859375, 'eval_loss': 0.31664034724235535}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RpchDiofwyn"
      },
      "source": [
        "Let's see how it performs on our test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuR5s85bfwyn",
        "outputId": "066f0bae-9d32-4a0f-9598-f6e2a6ea24a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "preds = trainer.predict(test_dataset)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:306] 2021-06-16 06:53:18,239 >> ***** Running Prediction *****\n",
            "[INFO|trainer_tf.py:307] 2021-06-16 06:53:18,243 >>   Num examples in dataset = 200\n",
            "[INFO|trainer_tf.py:310] 2021-06-16 06:53:18,245 >>   Batch size = 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZEYjvu9fwyo"
      },
      "source": [
        "The output from predict is logits, so we need to use a softmax to turn the values to probabilities and then use np.argmax to select the label with largest probalities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBR1ShX1fwyp"
      },
      "source": [
        "tf_predictions = tf.nn.softmax(preds.predictions, axis=-1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1owmeNfwyp"
      },
      "source": [
        "y_preds = np.argmax(tf_predictions, axis=-1)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkc2jJDnfwyp",
        "outputId": "c41f4c09-aa5a-4c7b-ae3a-bbc065e8d99e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(preds.label_ids, y_preds))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90       107\n",
            "           1       0.91      0.84      0.87        93\n",
            "\n",
            "    accuracy                           0.89       200\n",
            "   macro avg       0.89      0.88      0.88       200\n",
            "weighted avg       0.89      0.89      0.88       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8obURu4fwyp"
      },
      "source": [
        "#model.save_pretrained('./save_model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMSvuVBfwyp"
      },
      "source": [
        "## Try out the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZXWuxUufwyp"
      },
      "source": [
        "Now let's try out our model with our own sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBzO0YMfwyq",
        "outputId": "9657c166-8b56-4eb8-8f93-ddee488d7bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_sentence = \"I don't see how people can sit through this hour-long movie!\"\n",
        "#test_sentence = \"The movie, though flawed, is still interesting enough.\"\n",
        "test_sentence = 'The movie kept we on my toe all the time.'\n",
        "inputs = tokenizer(test_sentence, return_tensors=\"tf\")\n",
        "out = model(inputs)\n",
        "print(out)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.06228681,  0.01608959]], dtype=float32)>, hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBwdTBxxmgPK",
        "outputId": "83dfd3fb-31b0-4526-906b-d8bc3fcff4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(np.argmax(tf.nn.softmax(out.logits, axis=-1)))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0X1-Dtfwyq"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "- Try to use BERT base-cased pretrained model and see if you get better or worse performance.\n",
        "- Try to use BERT base-uncased pretrained model and see if you get better or worse performance.\n",
        "- Try using a larger number of training samples. \n",
        "- Try multi-class classification using the this [dataset](https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/news.csv) that groups news title into 4 categories: e (entertainment), b (business), t (tech), m (medical/health). Original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdcjl7t2fwyq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}