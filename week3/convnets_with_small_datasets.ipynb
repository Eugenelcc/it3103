{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convnets-with-small-datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python3.8 (tf2env)",
      "language": "python",
      "name": "tf2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week3/convnets_with_small_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZIgnFFEv5t"
      },
      "source": [
        "# Lab Exercise: Image Classification using Convolutional Neural Network \n",
        "\n",
        "In this practical, we will see how we can use a Convolutional Neural Network to classify cat and dog images.\n",
        "\n",
        "We will train the network using relatively little data (about 2000 images) which is a common real problem with a lot of deep learning projects where data is hard to come by. We have learnt in the lecture how we can solve the small data problem with some common techniques like data augmentation and transfer learning. We will examine how to use data augmentation in this lab and in the next lab, we will learn to use transfer learning.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQgbpqHw-vXy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQUHIqs-vX6"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "The cats vs. dogs dataset is available at Kaggle.com as part of a computer vision \n",
        "competition in late 2013. You can download the [original dataset](\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data) from Kaggle (you will need to create a Kaggle account if you don't already have one)\n",
        "\n",
        "The pictures are medium-resolution color JPEGs and are of various sizes and shapes that look like this:\n",
        "\n",
        "<img src='https://sdaai-public.s3-ap-southeast-1.amazonaws.com/it3103/images/cats_vs_dogs_samples.jpeg' height='300'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76mWqf7-vX6"
      },
      "source": [
        "This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). For the purpose of demonstrating challenges of training with small data set and also to have an opportunity to see the effects of using data augmentation technique, we will use a smaller subset (2000 train images and 1000 validation images) which you can download from [here](https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_filtered.zip). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSQZGCRtReJY"
      },
      "source": [
        "In the codes below, we use the keras ``get_file()`` utility to download and unzip the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grI88iK2-vX6"
      },
      "source": [
        "import os, shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JRhg0UBDRT1"
      },
      "source": [
        "dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "print(path_to_zip)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phy4xWMT-vX7"
      },
      "source": [
        "# Directories for our training,\n",
        "# train and validation splits\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYJ9QpHC-vX9"
      },
      "source": [
        "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvIwWbb-vX_"
      },
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaFjM9s-vX_"
      },
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVjMyPmZ-vYA"
      },
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rXZ-dlU-vYA"
      },
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chQJRgxL-vYB"
      },
      "source": [
        "\n",
        "So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of \n",
        "samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate \n",
        "measure of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXLfrh6-vYB"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "Our convnet will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
        "\n",
        "**Exercise 1**: \n",
        "\n",
        "Write the codes to implement the following: \n",
        "\n",
        "- Input layer should be of shape (150,150,3)\n",
        "- The hidden layers consist of the following Conv2D/MaxPooling2D blocks:\n",
        "  - Block 1: Conv layer with 32 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 2: Conv layer with 64 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 3/4: Conv layer with 128 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - a layer to convert 2D to 1D \n",
        "  - a Dropout layer to help to reduce overfitting\n",
        "- a Dense Layer as the output layer\n",
        "\n",
        "Use RELU as activation functions for all hidden layers. \n",
        "\n",
        "What activation function should you use for the output layer?\n",
        "\n",
        "<br/>\n",
        "\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6zYJGn-vYB"
      },
      "source": [
        "### TODO: Write the code to build the model and compile the model \n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = None\n",
        " "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EN4zoVi-vYC"
      },
      "source": [
        "Let's print the model summary to show the shape and paramater numbers for each layer. Your output should look something like this: \n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/expected_convnet_summary.png\" width=400 />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FJD8b_2-vYC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBfX0po-vYC"
      },
      "source": [
        "**Exercise 2**: \n",
        "\n",
        "Compile your model with the appropriate optimizer and loss function. We will use RMSProp with learning rate of 1e-4 and monitor the 'accuracy' metrics. What should we use for the loss function? \n",
        "\n",
        "Complete the code below. \n",
        "\n",
        "<br/>\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOdPRQ8j-vYC"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "### TODO: Complete the code below ####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We2Khkc-vYC"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Image data should be formatted into appropriately pre-processed floating point tensors before being fed into our \n",
        "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n",
        "\n",
        "* Read the picture files.\n",
        "* Decode the JPEG content to RGB grids of pixels.\n",
        "* Resize the image into same size (in our case, we will use 150 by 150)\n",
        "* Convert these into floating point tensors.\n",
        "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
        "\n",
        "It may seem a bit daunting, but tf.keras provides the class `ImageDataGenerator` which allows to \n",
        "quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7_Oujp-vYD"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzFJ1ptz-vYD"
      },
      "source": [
        "Let's take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape `(20, 150, 150, 3)`) and binary \n",
        "labels (shape `(20,)`). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches \n",
        "indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to `break` the iteration loop \n",
        "at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJxIZGy-vYD"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOx2NhAYQ0XF"
      },
      "source": [
        "How do we know what label is assigned to each of the class? We can use class_indices of the ImageGenerator to show the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fifOVDCQwva"
      },
      "source": [
        "validation_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4YMOgW8REAx"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lt_DbKQ-vYD"
      },
      "source": [
        "Let's fit our model to the data using the generator. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does. \n",
        "Because the data is being generated endlessly, the generator needs to know how many samples to draw from the generator before \n",
        "declaring an epoch over. This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the \n",
        "generator, i.e. after having run for `steps_per_epoch` gradient descent steps, the fitting process will go to the next epoch. In our case, \n",
        "batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.\n",
        "\n",
        "When using `fit`, one may pass a `validation_data`.Importantly, this argument is \n",
        "allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as `validation_data`, then \n",
        "this generator is expected to yield batches of validation data endlessly, and thus you should also specify the `validation_steps` argument, \n",
        "which tells the process how many batches to draw from the validation generator for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwyFdTu8-vYD"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej84-o7v-vYE"
      },
      "source": [
        "It is good practice to always save your models after training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btxvjweb-vYE"
      },
      "source": [
        "model.save('cats_and_dogs_small_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS2NA64l-vYE"
      },
      "source": [
        "Let's plot the loss and accuracy of the model over the training and validation data during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eje5UEye-vYE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZTbSxL-vYE"
      },
      "source": [
        "These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our \n",
        "validation accuracy stalls at 70-72%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss \n",
        "keeps decreasing linearly until it reaches nearly 0.\n",
        "\n",
        "Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. There are a \n",
        "number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to \n",
        "use one, specific to computer vision, and used almost universally when processing images with deep learning models: *data \n",
        "augmentation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9YQpQ8-vYE"
      },
      "source": [
        "## Using data augmentation\n",
        "\n",
        "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. \n",
        "Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data \n",
        "augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number \n",
        "of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same \n",
        "picture twice. This helps the model get exposed to more aspects of the data and generalize better.\n",
        "\n",
        "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` \n",
        "instance. Let's get started with an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbAfiBdu-vYE"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qiDkZc-vYF"
      },
      "source": [
        "These are just a few of the options available (for more, see the Keras documentation). Let's quickly go over what we just wrote:\n",
        "\n",
        "* `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures.\n",
        "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures \n",
        "vertically or horizontally.\n",
        "* `shear_range` is for randomly applying shearing transformations.\n",
        "* `zoom_range` is for randomly zooming inside pictures.\n",
        "* `horizontal_flip` is for randomly flipping half of the images horizontally -- relevant when there are no assumptions of horizontal \n",
        "asymmetry (e.g. real-world pictures).\n",
        "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
        "\n",
        "Let's take a look at our augmented images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLPiFmw-vYF"
      },
      "source": [
        "# This is module with image preprocessing utilities\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "\n",
        "# We pick one image to \"augment\"\n",
        "img_path = fnames[3]\n",
        "\n",
        "# Read the image and resize it\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "# Convert it to a Numpy array with shape (150, 150, 3)\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Reshape it to (1, 150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# The .flow() command below generates batches of randomly transformed images.\n",
        "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I6r5H9B-vYF"
      },
      "source": [
        "If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs \n",
        "that it sees are still heavily intercorrelated, since they come from a small number of original images -- we cannot produce new information, \n",
        "we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight \n",
        "overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier:\n",
        "\n",
        "**Exercise 3:**\n",
        "\n",
        "Modify your model to include the Dropout layer, using 50% dropout. \n",
        "\n",
        "<br/>\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gngudl4W-vYF"
      },
      "source": [
        "## TODO: create a new model with the Dropout layer ##\n",
        "\n",
        "model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFPI_eAP-vYF"
      },
      "source": [
        "Let's train our network using data augmentation and dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iXF1RZ5-vYF"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyPP5dAo-vYG"
      },
      "source": [
        "Let's save our model -- we will be using it in the section on convnet visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XSE6zLNG-vYG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "model.save('cats_and_dogs_small_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZY8r1O-vYG"
      },
      "source": [
        "Let's plot our results again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LZDuZaW-vYG"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLypNoy-vYG"
      },
      "source": [
        "Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation \n",
        "curves. We are now able to reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.\n",
        "\n",
        "By leveraging regularization techniques even further and by tuning the network's parameters (such as the number of filters per convolution \n",
        "layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86-87%. However, it would prove \n",
        "very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a \n",
        "next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two \n",
        "sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6kjxaztFdbn"
      },
      "source": [
        "## Test with our own image\n",
        "\n",
        "Now we are ready to put our trained model to test! \n",
        "You can upload any cat and dog image from your local computer using the code below.  The upload file will then be pre-processed into image tensor before feeding into our model for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go15_WVFGiZX"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0mLBA5CNnu4"
      },
      "source": [
        "# We need to pre-process our image to the shape expected by our model \n",
        "\n",
        "img = keras.preprocessing.image.load_img(\n",
        "    fn, target_size=(150, 150)\n",
        ")\n",
        "\n",
        "# we convert the image to numpy array\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# Although we only have single image, however our model expected data in batches\n",
        "# so we will need to add in the batch axis too\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "# we load the model saved earlier and do the inference \n",
        "model = tf.keras.models.load_model('cats_and_dogs_small_2')\n",
        "predictions = model(img_array)\n",
        "if predictions[0] > 0.5: \n",
        "    print('It is a dog')\n",
        "else:\n",
        "    print('It is a cat')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}