{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "token_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/token_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXdc1O6Gm-a"
      },
      "source": [
        "# Token Classification (Named Entity Recognition)\n",
        "\n",
        "In this practical we will learn how to use the HuggingFace Transformers library to perform token classification.\n",
        "\n",
        "Just like what we did in Practical 3a, we will use the DistiBERT transformer architecture, which also allows us to classify each and every word in a sentence.\n",
        "\n",
        "####**NOTE: Be sure to set your runtime to a GPU instance!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy827DuG3b7"
      },
      "source": [
        "## Install Transformers\n",
        "\n",
        "Run the following cell to install the HuggingFace Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-D3yf5P0rWz",
        "outputId": "9b493dc1-dba0-4f89-870f-418cf0bbcf98"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apSbg63-PhYf"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0wg6EKWOMhj",
        "outputId": "12040833-2864-4f1e-df73-07e0f18f008f"
      },
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_train.txt\n",
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-18 13:13:14--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_train.txt\n",
            "Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.32.47\n",
            "Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.32.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283418 (3.1M) [text/plain]\n",
            "Saving to: ‘token_train.txt.1’\n",
            "\n",
            "token_train.txt.1   100%[===================>]   3.13M  2.65MB/s    in 1.2s    \n",
            "\n",
            "2021-06-18 13:13:16 (2.65 MB/s) - ‘token_train.txt.1’ saved [3283418/3283418]\n",
            "\n",
            "--2021-06-18 13:13:16--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_test.txt\n",
            "Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.32.47\n",
            "Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.32.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748093 (731K) [text/plain]\n",
            "Saving to: ‘token_test.txt.1’\n",
            "\n",
            "token_test.txt.1    100%[===================>] 730.56K   872KB/s    in 0.8s    \n",
            "\n",
            "2021-06-18 13:13:18 (872 KB/s) - ‘token_test.txt.1’ saved [748093/748093]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8paYUgs38H5"
      },
      "source": [
        "## Process the data \n",
        "\n",
        "The data file is in CoNLL format: \n",
        "\n",
        "```\n",
        "sentence1-word1  PPTag-1-1  ChunkTag-1-1  NERTag-1-1\n",
        "sentence1-word2  PPTag-1-2  ChunkTag-1-2  NERTag-1-2\n",
        "sentence1-word3  PPTag-1-3  ChunkTag-1-3  NERTag-1-3\n",
        "<empty line>\n",
        "sentence2-word1  PPTag-2-1  ChunkTag-2-1  NERTag-2-1\n",
        "sentence2-word2  PPTag-2-2  ChunkTag-2-2  NERTag-2-2\n",
        "...\n",
        "sentence2-wordn  PPTag-2-n  ChunkTag-2-n  NERTag-2-n\n",
        "<empty line>\n",
        "...\n",
        "```\n",
        "\n",
        "For example, the sentence \"U.N. official Ekeus heads for Baghdad.\" will be represented as follow in CoNLL format: \n",
        "\n",
        "```\n",
        "U.N.      NNP  I-NP  I-ORG\n",
        "official  NN   I-NP  O\n",
        "Ekeus     NNP  I-NP  I-PER\n",
        "heads     VBZ  I-VP  O\n",
        "for       IN   I-PP  O\n",
        "Baghdad   NNP  I-NP  I-LOC\n",
        ".         .    O     O\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2aiV0haVvI-"
      },
      "source": [
        "We define a function to read the data file line by line and combined lines that belong to a sentence into a list of words and list of tags. \n",
        "\n",
        "As we are only interested in the Named Entity Recognition (NER) tags, we will only extract tags from column_index 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7mWDJnwTy9S"
      },
      "source": [
        "# This function returns a 2D list of words and a 2D list of labels\n",
        "# corresponding to each word.\n",
        "\n",
        "def load_conll(filepath, delimiter=' ', word_column_index=0, label_column_index=3):\n",
        "    all_texts = []\n",
        "    all_tags = []\n",
        "\n",
        "    texts = []\n",
        "    tags = []\n",
        "\n",
        "    # Opens the file.\n",
        "    #\n",
        "    with open(filepath, \"r\") as f:\n",
        "\n",
        "        # Loops through each line \n",
        "        for line in f:\n",
        "\n",
        "            # Split each line by its delimiter (default is a space)\n",
        "            tokens = line.split(delimiter)\n",
        "\n",
        "            # If the line is empty, treat it as the end of the\n",
        "            # previous sentence, and construct a new sentence\n",
        "            #\n",
        "            if len(tokens) == 1:\n",
        "                # Append the sentence\n",
        "                # \n",
        "                all_texts.append(texts)\n",
        "                all_tags.append(tags)\n",
        "\n",
        "                # Create a new sentence\n",
        "                #\n",
        "                texts = []\n",
        "                tags = []\n",
        "            else:\n",
        "                # Not yet end of the sentence, continue to add\n",
        "                # words into the current sentence\n",
        "                #\n",
        "                thistext = tokens[word_column_index].replace('\\n', '')\n",
        "                thistag = tokens[label_column_index].replace('\\n', '')\n",
        "\n",
        "                texts.append(thistext)\n",
        "                tags.append(thistag)\n",
        "\n",
        "    # Insert the last sentence if it contains at least 1 word.\n",
        "    #\n",
        "    if len(texts) > 0:\n",
        "        all_texts.append(texts)\n",
        "        all_tags.append(tags)\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return all_texts, all_tags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjtIJjOWWrs"
      },
      "source": [
        "We will now process our files with the function and examine the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgARQPbyWdDh"
      },
      "source": [
        "train_texts, train_tags = load_conll(\"token_train.txt\")\n",
        "val_texts, val_tags = load_conll(\"token_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7O43-HxWlmn",
        "outputId": "3857bb4d-947b-437c-b90a-8c3af3fc2221"
      },
      "source": [
        "print(train_texts[:3])\n",
        "print(train_tags[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-DOCSTART-'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn']]\n",
            "[['O'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVm7WR1kXm2F"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Now we have our texts and labels. Before we can feed the texts and labels into our model for training, we need to tokenize our texts and also encode our labels into numeric forms.\n",
        "\n",
        "We first define the token labels we need and define the mapping to a numeric index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOghzNYqE5v"
      },
      "source": [
        "# Define a list of unique token labels that we will recognize\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Create a reverse-mapping dictionary of the label -> index.\n",
        "#\n",
        "token_labels_id_by_label = {tag: id for id, tag in enumerate(token_labels)}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQNRo7B3EidZ"
      },
      "source": [
        "\n",
        "We will now need to tokenize our text.  Let's look at a potential problem that can happen we do tokenization. Transformers model like Bert uses WordPiece Tokenization, meaning that single words are split into multiple tokens (this is done to solve the out-of-vocabulary problem for rare words). For example, DistilBert’s tokenizer would split the `[\"Nadim\", \"Ladki\"]` into the tokens `[[CLS], \"na\", \"##im\",\"lad\", ##ki\", [SEP]]`. This is a problem for us because we have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels.\n",
        "\n",
        "Before tokenization with WordPiece, it is one to one matching between tokens and tags:\n",
        "\n",
        "```\n",
        "tokens = [\"Nadim\", \"Ladki\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "After tokenization with WordPiece, there is no more one-to-one match between them: \n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in Transformers by setting the labels we wish to ignore to -100. We will also ignore special tokens like `[CLS]` and `[SEP]`. In the example above, if the label for 'Nadim' 1 (index for B-PER) and 'Ladki' is 2 (index for I-PER), we would set the labels as follows: \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = [-100, 1, -100, 2, -100, -100]\n",
        "```\n",
        "\n",
        "But how do we know which token to ignore? This is where we need to use the offset_mapping from the tokenizer. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token’s start position and end position relative to the original token it was split from. \n",
        "\n",
        "For example, in the origial token 'nadim', subtoken \"##dim\" is starts at original position 3 (i.e. `d`) and ends in position 5 (i.e. `m`). So the offset_mapping for `##dim` thus is given as `(3,5)`. Also, you can see that the special tokens like `[CLS]` has a offset_mapping of `(0,0)`. \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "offset_mappings = [(0, 0), (0, 3), (3, 5), (0, 3), (3, 5), (0, 0)]\n",
        "```\n",
        "\n",
        "That means that if the first position in the tuple is anything other than 0, we will set its corresponding label to -100. While we’re at it, we can also set labels to -100 if the second position of the offset mapping is 0, since this means it must be a special token like `[SEP]` or `[CLS]`.\n",
        "\n",
        "The following function `encode_tags()` takes in original tags and encode it according to the logic described above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyvtz395c0ir"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[token_labels_id_by_label[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT5Wu4sFYorz"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the DistilBERT tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# define a reverse lookup table for mapping id to corresponding word\n",
        "index2word = { value: key for key, value in tokenizer.get_vocab().items() }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJx99lYbMfC6"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, \n",
        "                            is_split_into_words=True, \n",
        "                            return_offsets_mapping=True, \n",
        "                            padding=True, \n",
        "                            truncation=True)\n",
        "val_encodings = tokenizer(val_texts, \n",
        "                          is_split_into_words=True, \n",
        "                          return_offsets_mapping=True, \n",
        "                          padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXImpLVMqzo"
      },
      "source": [
        "Let's examine the encoding of one sample. Since we set `return_offsets_mapping` to `True`, we will see the offset_mapping in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6AVs9LNMngq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c1fa29-bb5a-4de1-dc1b-9dd25ab1e129"
      },
      "source": [
        "print(train_encodings.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS3TixmANXtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd409fcd-308f-4f93-ac05-29f6cd5beaf0"
      },
      "source": [
        "for i in range(5):\n",
        "  print([index2word[id] for id in train_encodings.input_ids[i] if id != 0])\n",
        "  print(train_encodings.offset_mapping[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '-', 'doc', '##star', '##t', '-', '[SEP]']\n",
            "[(0, 0), (0, 1), (1, 4), (4, 8), (8, 9), (9, 10), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n",
            "[(0, 0), (0, 2), (0, 7), (0, 6), (0, 4), (0, 2), (0, 7), (0, 7), (0, 4), (0, 1), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "['[CLS]', 'peter', 'blackburn', '[SEP]']\n",
            "[(0, 0), (0, 5), (0, 9), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "['[CLS]', 'brussels', '1996', '-', '08', '-', '22', '[SEP]']\n",
            "[(0, 0), (0, 8), (0, 4), (4, 5), (5, 7), (7, 8), (8, 10), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "['[CLS]', 'the', 'european', 'commission', 'said', 'on', 'thursday', 'it', 'disagreed', 'with', 'german', 'advice', 'to', 'consumers', 'to', 'shu', '##n', 'british', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', '[SEP]']\n",
            "[(0, 0), (0, 3), (0, 8), (0, 10), (0, 4), (0, 2), (0, 8), (0, 2), (0, 9), (0, 4), (0, 6), (0, 6), (0, 2), (0, 9), (0, 2), (0, 3), (3, 4), (0, 7), (0, 4), (0, 5), (0, 10), (0, 9), (0, 7), (0, 3), (0, 3), (0, 7), (0, 3), (0, 2), (0, 11), (0, 2), (0, 5), (0, 1), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LuZ9ovPRFDl"
      },
      "source": [
        "Now we will go ahead and encode our tag labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWovfkggRK7a"
      },
      "source": [
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAonFBXU5EWX"
      },
      "source": [
        "Now we are ready to create our datasets for training and evaluating our models. Before that we need to remove offset_mapping from the encodings as it is not needed by our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpwVcZEGpnv9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxisdqSj36C-"
      },
      "source": [
        "Run the following cell below to see the first few samples of the train dataset to see if they looks all right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu7FKTq7mEbo",
        "outputId": "3dfc7387-7249-4009-aca6-644f763a1cad"
      },
      "source": [
        "iterator = iter(train_dataset)\n",
        "\n",
        "for i in range(3):\n",
        "    print (train_texts[i])\n",
        "    print(iterator.get_next())\n",
        "    print (\"---\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-DOCSTART-']\n",
            "({'input_ids': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([  101,  1011,  9986, 14117,  2102,  1011,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32)>)\n",
            "---\n",
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "({'input_ids': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([  101,  7327, 19164,  2446,  2655,  2000, 17757,  2329, 12559,\n",
            "        1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([-100,    3,    0,    7,    0,    0,    0,    7,    0,    0, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32)>)\n",
            "---\n",
            "['Peter', 'Blackburn']\n",
            "({'input_ids': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([  101,  2848, 13934,   102,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(164,), dtype=int32, numpy=\n",
            "array([-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "      dtype=int32)>)\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDGUmBIPfp4"
      },
      "source": [
        "## Train our Token Classification Model\n",
        "\n",
        "We will now set up the training configuration. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOcCiXN-06lx",
        "outputId": "265936b6-02bc-4ac2-a44e-6a2bc3c558aa"
      },
      "source": [
        "from transformers import (\n",
        "    TFAutoModelForTokenClassification, \n",
        "    TFTrainer, \n",
        "    TFTrainingArguments\n",
        ")\n",
        "\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    token_model = TFAutoModelForTokenClassification.from_pretrained('distilbert-base-uncased', \n",
        "                                                              num_labels=len(token_labels))\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=token_model,                   # the instantiated Token Classification Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|training_args.py:725] 2021-06-18 13:14:07,706 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:625] 2021-06-18 13:14:07,712 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[INFO|training_args_tf.py:192] 2021-06-18 13:14:07,715 >> Tensorflow: setting up strategy\n",
            "[INFO|configuration_utils.py:517] 2021-06-18 13:14:08,012 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:553] 2021-06-18 13:14:08,014 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.7.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_tf_utils.py:1261] 2021-06-18 13:14:08,319 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/fa107dc22c014df078a1b75235144a927f7e9764916222711f239b7ee6092ec9.bc4b731be56d8422e12b1d5bfa86fbd81d18d2770da1f5ac4f33640a17b7dde9.h5\n",
            "[WARNING|modeling_tf_utils.py:1311] 2021-06-18 13:14:09,370 >> Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_tf_utils.py:1323] 2021-06-18 13:14:09,371 >> Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|trainer_tf.py:117] 2021-06-18 13:14:09,383 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "[INFO|trainer_tf.py:125] 2021-06-18 13:14:09,384 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEGEiLiULBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e7fd3b-70bc-4801-da28-3ca1b9d4ca44"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:528] 2021-06-18 13:14:15,488 >> ***** Running training *****\n",
            "[INFO|trainer_tf.py:529] 2021-06-18 13:14:15,489 >>   Num examples = 14987\n",
            "[INFO|trainer_tf.py:531] 2021-06-18 13:14:15,490 >>   Num Epochs = 2.0\n",
            "[INFO|trainer_tf.py:532] 2021-06-18 13:14:15,492 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer_tf.py:534] 2021-06-18 13:14:15,495 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer_tf.py:536] 2021-06-18 13:14:15,497 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer_tf.py:537] 2021-06-18 13:14:15,499 >>   Steps per epoch = 937\n",
            "[INFO|trainer_tf.py:538] 2021-06-18 13:14:15,500 >>   Total optimization steps = 1874\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb6b6b90de0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb6b6b90de0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fb6d243edd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fb6d243edd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO|trainer_tf.py:404] 2021-06-18 13:14:59,724 >> {'loss': 2.1983075, 'learning_rate': 1e-06, 'epoch': 0.010672358591248666, 'step': 10}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:02,422 >> {'loss': 2.1826541, 'learning_rate': 2e-06, 'epoch': 0.021344717182497332, 'step': 20}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:05,123 >> {'loss': 2.1378436, 'learning_rate': 2.9999999e-06, 'epoch': 0.032017075773746, 'step': 30}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:07,850 >> {'loss': 2.07803, 'learning_rate': 4e-06, 'epoch': 0.042689434364994665, 'step': 40}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:10,565 >> {'loss': 1.9951329, 'learning_rate': 5.0000003e-06, 'epoch': 0.05336179295624333, 'step': 50}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:13,301 >> {'loss': 1.8849933, 'learning_rate': 5.9999998e-06, 'epoch': 0.064034151547492, 'step': 60}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:16,039 >> {'loss': 1.7656013, 'learning_rate': 6.9999996e-06, 'epoch': 0.07470651013874066, 'step': 70}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:18,797 >> {'loss': 1.645985, 'learning_rate': 8e-06, 'epoch': 0.08537886872998933, 'step': 80}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:21,567 >> {'loss': 1.5517366, 'learning_rate': 9e-06, 'epoch': 0.096051227321238, 'step': 90}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:24,354 >> {'loss': 1.4580698, 'learning_rate': 1.0000001e-05, 'epoch': 0.10672358591248667, 'step': 100}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:27,126 >> {'loss': 1.3850617, 'learning_rate': 1.1e-05, 'epoch': 0.11739594450373532, 'step': 110}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:29,914 >> {'loss': 1.3164268, 'learning_rate': 1.19999995e-05, 'epoch': 0.128068303094984, 'step': 120}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:32,701 >> {'loss': 1.2532697, 'learning_rate': 1.2999999e-05, 'epoch': 0.13874066168623267, 'step': 130}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:35,489 >> {'loss': 1.1980228, 'learning_rate': 1.3999999e-05, 'epoch': 0.14941302027748132, 'step': 140}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:38,265 >> {'loss': 1.1490215, 'learning_rate': 1.5000001e-05, 'epoch': 0.16008537886872998, 'step': 150}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:41,027 >> {'loss': 1.1011224, 'learning_rate': 1.6e-05, 'epoch': 0.17075773745997866, 'step': 160}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:43,787 >> {'loss': 1.059154, 'learning_rate': 1.7e-05, 'epoch': 0.1814300960512273, 'step': 170}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:46,544 >> {'loss': 1.020836, 'learning_rate': 1.8e-05, 'epoch': 0.192102454642476, 'step': 180}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:49,287 >> {'loss': 0.98341155, 'learning_rate': 1.9e-05, 'epoch': 0.20277481323372465, 'step': 190}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:52,019 >> {'loss': 0.9501861, 'learning_rate': 2e-05, 'epoch': 0.21344717182497333, 'step': 200}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:54,748 >> {'loss': 0.91767323, 'learning_rate': 2.0999998e-05, 'epoch': 0.22411953041622198, 'step': 210}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:15:57,476 >> {'loss': 0.8855232, 'learning_rate': 2.2e-05, 'epoch': 0.23479188900747064, 'step': 220}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:00,194 >> {'loss': 0.85819954, 'learning_rate': 2.3e-05, 'epoch': 0.24546424759871932, 'step': 230}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:02,914 >> {'loss': 0.8333057, 'learning_rate': 2.3999999e-05, 'epoch': 0.256136606189968, 'step': 240}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:05,645 >> {'loss': 0.80824906, 'learning_rate': 2.5e-05, 'epoch': 0.26680896478121663, 'step': 250}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:08,358 >> {'loss': 0.7833255, 'learning_rate': 2.5999998e-05, 'epoch': 0.27748132337246534, 'step': 260}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:11,071 >> {'loss': 0.7613404, 'learning_rate': 2.7e-05, 'epoch': 0.288153681963714, 'step': 270}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:13,783 >> {'loss': 0.7400701, 'learning_rate': 2.7999999e-05, 'epoch': 0.29882604055496265, 'step': 280}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:16,499 >> {'loss': 0.720821, 'learning_rate': 2.8999999e-05, 'epoch': 0.3094983991462113, 'step': 290}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:19,207 >> {'loss': 0.7029496, 'learning_rate': 3.0000001e-05, 'epoch': 0.32017075773745995, 'step': 300}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:21,923 >> {'loss': 0.6844127, 'learning_rate': 3.1e-05, 'epoch': 0.33084311632870866, 'step': 310}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:24,638 >> {'loss': 0.66762215, 'learning_rate': 3.2e-05, 'epoch': 0.3415154749199573, 'step': 320}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:27,350 >> {'loss': 0.65056497, 'learning_rate': 3.3e-05, 'epoch': 0.35218783351120597, 'step': 330}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:30,065 >> {'loss': 0.63498604, 'learning_rate': 3.4e-05, 'epoch': 0.3628601921024546, 'step': 340}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:32,786 >> {'loss': 0.61941785, 'learning_rate': 3.4999997e-05, 'epoch': 0.37353255069370334, 'step': 350}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:35,507 >> {'loss': 0.60569316, 'learning_rate': 3.6e-05, 'epoch': 0.384204909284952, 'step': 360}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:38,228 >> {'loss': 0.59230155, 'learning_rate': 3.6999998e-05, 'epoch': 0.39487726787620064, 'step': 370}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:40,961 >> {'loss': 0.5791439, 'learning_rate': 3.8e-05, 'epoch': 0.4055496264674493, 'step': 380}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:43,700 >> {'loss': 0.56709045, 'learning_rate': 3.8999995e-05, 'epoch': 0.41622198505869795, 'step': 390}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:46,431 >> {'loss': 0.55555904, 'learning_rate': 4e-05, 'epoch': 0.42689434364994666, 'step': 400}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:49,175 >> {'loss': 0.54504377, 'learning_rate': 4.1e-05, 'epoch': 0.4375667022411953, 'step': 410}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:51,916 >> {'loss': 0.5344348, 'learning_rate': 4.1999996e-05, 'epoch': 0.44823906083244397, 'step': 420}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:54,661 >> {'loss': 0.524146, 'learning_rate': 4.2999996e-05, 'epoch': 0.4589114194236926, 'step': 430}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:16:57,414 >> {'loss': 0.51477444, 'learning_rate': 4.4e-05, 'epoch': 0.4695837780149413, 'step': 440}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:00,159 >> {'loss': 0.50494987, 'learning_rate': 4.4999997e-05, 'epoch': 0.48025613660619, 'step': 450}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:02,903 >> {'loss': 0.49614668, 'learning_rate': 4.6e-05, 'epoch': 0.49092849519743864, 'step': 460}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:05,648 >> {'loss': 0.48724505, 'learning_rate': 4.6999998e-05, 'epoch': 0.5016008537886874, 'step': 470}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:08,396 >> {'loss': 0.47841263, 'learning_rate': 4.7999998e-05, 'epoch': 0.512273212379936, 'step': 480}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:11,145 >> {'loss': 0.47083297, 'learning_rate': 4.9e-05, 'epoch': 0.5229455709711847, 'step': 490}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:13,889 >> {'loss': 0.46315685, 'learning_rate': 5e-05, 'epoch': 0.5336179295624333, 'step': 500}\n",
            "[INFO|trainer_tf.py:595] 2021-06-18 13:17:17,366 >> Saving checkpoint for step 500 at ./results/checkpoint/ckpt-1\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:20,131 >> {'loss': 0.45595554, 'learning_rate': 4.9636095e-05, 'epoch': 0.544290288153682, 'step': 510}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:22,866 >> {'loss': 0.44841418, 'learning_rate': 4.9272196e-05, 'epoch': 0.5549626467449307, 'step': 520}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:25,584 >> {'loss': 0.4418751, 'learning_rate': 4.8908296e-05, 'epoch': 0.5656350053361793, 'step': 530}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:28,327 >> {'loss': 0.4354407, 'learning_rate': 4.8544392e-05, 'epoch': 0.576307363927428, 'step': 540}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:31,055 >> {'loss': 0.42869467, 'learning_rate': 4.8180493e-05, 'epoch': 0.5869797225186766, 'step': 550}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:33,782 >> {'loss': 0.42280245, 'learning_rate': 4.781659e-05, 'epoch': 0.5976520811099253, 'step': 560}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:36,517 >> {'loss': 0.41660792, 'learning_rate': 4.7452693e-05, 'epoch': 0.608324439701174, 'step': 570}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:39,250 >> {'loss': 0.4108818, 'learning_rate': 4.7088794e-05, 'epoch': 0.6189967982924226, 'step': 580}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:41,976 >> {'loss': 0.404975, 'learning_rate': 4.672489e-05, 'epoch': 0.6296691568836713, 'step': 590}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:44,705 >> {'loss': 0.39922053, 'learning_rate': 4.636099e-05, 'epoch': 0.6403415154749199, 'step': 600}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:47,443 >> {'loss': 0.39401704, 'learning_rate': 4.5997083e-05, 'epoch': 0.6510138740661686, 'step': 610}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:50,177 >> {'loss': 0.3889815, 'learning_rate': 4.5633184e-05, 'epoch': 0.6616862326574173, 'step': 620}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:52,908 >> {'loss': 0.3839232, 'learning_rate': 4.5269284e-05, 'epoch': 0.6723585912486659, 'step': 630}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:55,638 >> {'loss': 0.3790206, 'learning_rate': 4.4905384e-05, 'epoch': 0.6830309498399146, 'step': 640}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:17:58,368 >> {'loss': 0.37401628, 'learning_rate': 4.4541484e-05, 'epoch': 0.6937033084311632, 'step': 650}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:01,089 >> {'loss': 0.36925644, 'learning_rate': 4.417758e-05, 'epoch': 0.7043756670224119, 'step': 660}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:03,819 >> {'loss': 0.36455026, 'learning_rate': 4.381368e-05, 'epoch': 0.7150480256136607, 'step': 670}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:06,557 >> {'loss': 0.3600054, 'learning_rate': 4.344978e-05, 'epoch': 0.7257203842049093, 'step': 680}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:09,287 >> {'loss': 0.35573155, 'learning_rate': 4.3085878e-05, 'epoch': 0.736392742796158, 'step': 690}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:12,020 >> {'loss': 0.35135514, 'learning_rate': 4.272198e-05, 'epoch': 0.7470651013874067, 'step': 700}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:14,766 >> {'loss': 0.3472149, 'learning_rate': 4.235808e-05, 'epoch': 0.7577374599786553, 'step': 710}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:17,495 >> {'loss': 0.34336177, 'learning_rate': 4.199418e-05, 'epoch': 0.768409818569904, 'step': 720}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:22,963 >> {'loss': 0.33581656, 'learning_rate': 4.1266376e-05, 'epoch': 0.7897545357524013, 'step': 740}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:25,701 >> {'loss': 0.3322239, 'learning_rate': 4.090247e-05, 'epoch': 0.80042689434365, 'step': 750}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:28,435 >> {'loss': 0.32861596, 'learning_rate': 4.0538573e-05, 'epoch': 0.8110992529348986, 'step': 760}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:31,175 >> {'loss': 0.32502767, 'learning_rate': 4.017467e-05, 'epoch': 0.8217716115261473, 'step': 770}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:33,904 >> {'loss': 0.32150248, 'learning_rate': 3.981077e-05, 'epoch': 0.8324439701173959, 'step': 780}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:36,643 >> {'loss': 0.31845045, 'learning_rate': 3.944687e-05, 'epoch': 0.8431163287086446, 'step': 790}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:39,375 >> {'loss': 0.31528178, 'learning_rate': 3.9082963e-05, 'epoch': 0.8537886872998933, 'step': 800}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:42,110 >> {'loss': 0.3119547, 'learning_rate': 3.871907e-05, 'epoch': 0.8644610458911419, 'step': 810}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:44,844 >> {'loss': 0.308777, 'learning_rate': 3.8355163e-05, 'epoch': 0.8751334044823906, 'step': 820}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:47,584 >> {'loss': 0.30558994, 'learning_rate': 3.7991267e-05, 'epoch': 0.8858057630736392, 'step': 830}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:50,322 >> {'loss': 0.30273765, 'learning_rate': 3.7627364e-05, 'epoch': 0.8964781216648879, 'step': 840}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:53,057 >> {'loss': 0.299836, 'learning_rate': 3.7263464e-05, 'epoch': 0.9071504802561366, 'step': 850}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:55,786 >> {'loss': 0.2970052, 'learning_rate': 3.689956e-05, 'epoch': 0.9178228388473852, 'step': 860}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:18:58,519 >> {'loss': 0.29414374, 'learning_rate': 3.653566e-05, 'epoch': 0.928495197438634, 'step': 870}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:01,252 >> {'loss': 0.29151997, 'learning_rate': 3.617176e-05, 'epoch': 0.9391675560298826, 'step': 880}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:03,991 >> {'loss': 0.2888381, 'learning_rate': 3.5807858e-05, 'epoch': 0.9498399146211313, 'step': 890}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:06,726 >> {'loss': 0.28617692, 'learning_rate': 3.5443958e-05, 'epoch': 0.96051227321238, 'step': 900}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:09,463 >> {'loss': 0.28361905, 'learning_rate': 3.508006e-05, 'epoch': 0.9711846318036286, 'step': 910}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:12,205 >> {'loss': 0.28109795, 'learning_rate': 3.471616e-05, 'epoch': 0.9818569903948773, 'step': 920}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:14,940 >> {'loss': 0.27855653, 'learning_rate': 3.4352255e-05, 'epoch': 0.9925293489861259, 'step': 930}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:17,742 >> {'loss': 0.04580204, 'learning_rate': 3.3988355e-05, 'epoch': 1.0032017075773747, 'step': 940}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:20,477 >> {'loss': 0.03809374, 'learning_rate': 3.3624452e-05, 'epoch': 1.0138740661686232, 'step': 950}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:23,208 >> {'loss': 0.03995947, 'learning_rate': 3.326055e-05, 'epoch': 1.024546424759872, 'step': 960}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:25,933 >> {'loss': 0.042137455, 'learning_rate': 3.2896653e-05, 'epoch': 1.0352187833511206, 'step': 970}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:28,663 >> {'loss': 0.044885106, 'learning_rate': 3.253275e-05, 'epoch': 1.0458911419423693, 'step': 980}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:31,394 >> {'loss': 0.045209967, 'learning_rate': 3.216885e-05, 'epoch': 1.056563500533618, 'step': 990}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:34,118 >> {'loss': 0.043428436, 'learning_rate': 3.1804946e-05, 'epoch': 1.0672358591248665, 'step': 1000}\n",
            "[INFO|trainer_tf.py:595] 2021-06-18 13:19:37,270 >> Saving checkpoint for step 1000 at ./results/checkpoint/ckpt-2\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:40,050 >> {'loss': 0.041298784, 'learning_rate': 3.1441046e-05, 'epoch': 1.0779082177161152, 'step': 1010}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:42,770 >> {'loss': 0.04232816, 'learning_rate': 3.1077147e-05, 'epoch': 1.088580576307364, 'step': 1020}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:45,496 >> {'loss': 0.042032387, 'learning_rate': 3.0713243e-05, 'epoch': 1.0992529348986126, 'step': 1030}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:48,227 >> {'loss': 0.041850623, 'learning_rate': 3.0349343e-05, 'epoch': 1.1099252934898614, 'step': 1040}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:50,954 >> {'loss': 0.042251855, 'learning_rate': 2.9985446e-05, 'epoch': 1.1205976520811098, 'step': 1050}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:53,690 >> {'loss': 0.040584445, 'learning_rate': 2.962154e-05, 'epoch': 1.1312700106723586, 'step': 1060}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:56,433 >> {'loss': 0.040157296, 'learning_rate': 2.9257639e-05, 'epoch': 1.1419423692636073, 'step': 1070}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:19:59,173 >> {'loss': 0.040947672, 'learning_rate': 2.8893739e-05, 'epoch': 1.152614727854856, 'step': 1080}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:01,922 >> {'loss': 0.040965337, 'learning_rate': 2.852984e-05, 'epoch': 1.1632870864461047, 'step': 1090}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:04,664 >> {'loss': 0.040742487, 'learning_rate': 2.816594e-05, 'epoch': 1.1739594450373532, 'step': 1100}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:07,405 >> {'loss': 0.040215917, 'learning_rate': 2.7802038e-05, 'epoch': 1.1846318036286019, 'step': 1110}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:10,148 >> {'loss': 0.041091226, 'learning_rate': 2.7438136e-05, 'epoch': 1.1953041622198506, 'step': 1120}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:12,890 >> {'loss': 0.04117969, 'learning_rate': 2.7074235e-05, 'epoch': 1.2059765208110993, 'step': 1130}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:15,631 >> {'loss': 0.04084834, 'learning_rate': 2.6710333e-05, 'epoch': 1.216648879402348, 'step': 1140}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:18,366 >> {'loss': 0.0400967, 'learning_rate': 2.6346432e-05, 'epoch': 1.2273212379935965, 'step': 1150}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:21,100 >> {'loss': 0.040221322, 'learning_rate': 2.5982534e-05, 'epoch': 1.2379935965848452, 'step': 1160}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:23,840 >> {'loss': 0.039532863, 'learning_rate': 2.561863e-05, 'epoch': 1.248665955176094, 'step': 1170}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:26,576 >> {'loss': 0.039385952, 'learning_rate': 2.5254732e-05, 'epoch': 1.2593383137673426, 'step': 1180}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:29,311 >> {'loss': 0.039508972, 'learning_rate': 2.489083e-05, 'epoch': 1.2700106723585913, 'step': 1190}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:32,039 >> {'loss': 0.03951421, 'learning_rate': 2.4526928e-05, 'epoch': 1.2806830309498398, 'step': 1200}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:34,768 >> {'loss': 0.039334767, 'learning_rate': 2.4163028e-05, 'epoch': 1.2913553895410885, 'step': 1210}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:37,493 >> {'loss': 0.039138697, 'learning_rate': 2.3799126e-05, 'epoch': 1.3020277481323372, 'step': 1220}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:40,226 >> {'loss': 0.03916072, 'learning_rate': 2.3435225e-05, 'epoch': 1.312700106723586, 'step': 1230}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:42,956 >> {'loss': 0.039244544, 'learning_rate': 2.3071323e-05, 'epoch': 1.3233724653148347, 'step': 1240}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:45,679 >> {'loss': 0.03900234, 'learning_rate': 2.2707423e-05, 'epoch': 1.3340448239060834, 'step': 1250}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:48,406 >> {'loss': 0.03907215, 'learning_rate': 2.234352e-05, 'epoch': 1.3447171824973319, 'step': 1260}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:51,135 >> {'loss': 0.039157398, 'learning_rate': 2.1979622e-05, 'epoch': 1.3553895410885806, 'step': 1270}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:53,879 >> {'loss': 0.03876636, 'learning_rate': 2.1615719e-05, 'epoch': 1.3660618996798293, 'step': 1280}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:56,617 >> {'loss': 0.038481716, 'learning_rate': 2.1251819e-05, 'epoch': 1.376734258271078, 'step': 1290}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:20:59,347 >> {'loss': 0.038266167, 'learning_rate': 2.0887916e-05, 'epoch': 1.3874066168623265, 'step': 1300}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:02,073 >> {'loss': 0.038708042, 'learning_rate': 2.0524014e-05, 'epoch': 1.3980789754535752, 'step': 1310}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:04,811 >> {'loss': 0.039104097, 'learning_rate': 2.0160118e-05, 'epoch': 1.4087513340448239, 'step': 1320}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:07,540 >> {'loss': 0.03908729, 'learning_rate': 1.9796216e-05, 'epoch': 1.4194236926360726, 'step': 1330}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:10,272 >> {'loss': 0.038739998, 'learning_rate': 1.9432315e-05, 'epoch': 1.4300960512273213, 'step': 1340}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:13,003 >> {'loss': 0.038610537, 'learning_rate': 1.9068413e-05, 'epoch': 1.44076840981857, 'step': 1350}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:15,732 >> {'loss': 0.03810606, 'learning_rate': 1.8704512e-05, 'epoch': 1.4514407684098185, 'step': 1360}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:18,459 >> {'loss': 0.038153645, 'learning_rate': 1.8340612e-05, 'epoch': 1.4621131270010672, 'step': 1370}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:21,187 >> {'loss': 0.038002297, 'learning_rate': 1.797671e-05, 'epoch': 1.472785485592316, 'step': 1380}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:23,924 >> {'loss': 0.037872646, 'learning_rate': 1.7612809e-05, 'epoch': 1.4834578441835646, 'step': 1390}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:26,659 >> {'loss': 0.037449554, 'learning_rate': 1.7248907e-05, 'epoch': 1.4941302027748131, 'step': 1400}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:29,399 >> {'loss': 0.03740665, 'learning_rate': 1.6885006e-05, 'epoch': 1.5048025613660618, 'step': 1410}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:32,147 >> {'loss': 0.03704268, 'learning_rate': 1.6521104e-05, 'epoch': 1.5154749199573105, 'step': 1420}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:34,882 >> {'loss': 0.036860812, 'learning_rate': 1.6157202e-05, 'epoch': 1.5261472785485592, 'step': 1430}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:37,615 >> {'loss': 0.03661241, 'learning_rate': 1.5793305e-05, 'epoch': 1.536819637139808, 'step': 1440}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:40,349 >> {'loss': 0.036440186, 'learning_rate': 1.5429405e-05, 'epoch': 1.5474919957310567, 'step': 1450}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:43,090 >> {'loss': 0.03667641, 'learning_rate': 1.5065501e-05, 'epoch': 1.5581643543223054, 'step': 1460}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:45,824 >> {'loss': 0.036561787, 'learning_rate': 1.47016e-05, 'epoch': 1.5688367129135539, 'step': 1470}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:48,554 >> {'loss': 0.036526218, 'learning_rate': 1.43377e-05, 'epoch': 1.5795090715048026, 'step': 1480}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:51,289 >> {'loss': 0.036432702, 'learning_rate': 1.3973799e-05, 'epoch': 1.590181430096051, 'step': 1490}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:54,027 >> {'loss': 0.036714926, 'learning_rate': 1.3609898e-05, 'epoch': 1.6008537886872998, 'step': 1500}\n",
            "[INFO|trainer_tf.py:595] 2021-06-18 13:21:57,178 >> Saving checkpoint for step 1500 at ./results/checkpoint/ckpt-3\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:21:59,959 >> {'loss': 0.036506813, 'learning_rate': 1.3245996e-05, 'epoch': 1.6115261472785485, 'step': 1510}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:02,682 >> {'loss': 0.03619871, 'learning_rate': 1.2882096e-05, 'epoch': 1.6221985058697972, 'step': 1520}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:05,413 >> {'loss': 0.036080368, 'learning_rate': 1.2518194e-05, 'epoch': 1.632870864461046, 'step': 1530}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:08,137 >> {'loss': 0.035702247, 'learning_rate': 1.2154293e-05, 'epoch': 1.6435432230522946, 'step': 1540}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:10,879 >> {'loss': 0.035518438, 'learning_rate': 1.1790392e-05, 'epoch': 1.6542155816435433, 'step': 1550}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:13,626 >> {'loss': 0.03550773, 'learning_rate': 1.142649e-05, 'epoch': 1.664887940234792, 'step': 1560}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:16,366 >> {'loss': 0.035329636, 'learning_rate': 1.10625915e-05, 'epoch': 1.6755602988260405, 'step': 1570}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:19,101 >> {'loss': 0.035277132, 'learning_rate': 1.0698691e-05, 'epoch': 1.6862326574172892, 'step': 1580}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:21,845 >> {'loss': 0.03559415, 'learning_rate': 1.0334789e-05, 'epoch': 1.696905016008538, 'step': 1590}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:24,576 >> {'loss': 0.035339035, 'learning_rate': 9.970889e-06, 'epoch': 1.7075773745997864, 'step': 1600}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:27,312 >> {'loss': 0.035074674, 'learning_rate': 9.606987e-06, 'epoch': 1.7182497331910351, 'step': 1610}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:30,047 >> {'loss': 0.034879923, 'learning_rate': 9.2430855e-06, 'epoch': 1.7289220917822838, 'step': 1620}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:32,789 >> {'loss': 0.03484713, 'learning_rate': 8.879185e-06, 'epoch': 1.7395944503735326, 'step': 1630}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:35,524 >> {'loss': 0.035071634, 'learning_rate': 8.515282e-06, 'epoch': 1.7502668089647813, 'step': 1640}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:38,269 >> {'loss': 0.035005163, 'learning_rate': 8.151382e-06, 'epoch': 1.76093916755603, 'step': 1650}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:41,010 >> {'loss': 0.03506049, 'learning_rate': 7.787481e-06, 'epoch': 1.7716115261472787, 'step': 1660}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:43,751 >> {'loss': 0.035141926, 'learning_rate': 7.4235795e-06, 'epoch': 1.7822838847385272, 'step': 1670}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:46,485 >> {'loss': 0.034948613, 'learning_rate': 7.0596784e-06, 'epoch': 1.7929562433297759, 'step': 1680}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:49,210 >> {'loss': 0.03497802, 'learning_rate': 6.69578e-06, 'epoch': 1.8036286019210246, 'step': 1690}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:51,938 >> {'loss': 0.034898613, 'learning_rate': 6.331878e-06, 'epoch': 1.814300960512273, 'step': 1700}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:54,674 >> {'loss': 0.03477735, 'learning_rate': 5.967977e-06, 'epoch': 1.8249733191035218, 'step': 1710}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:22:57,402 >> {'loss': 0.034818776, 'learning_rate': 5.6040763e-06, 'epoch': 1.8356456776947705, 'step': 1720}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:00,131 >> {'loss': 0.034796026, 'learning_rate': 5.240175e-06, 'epoch': 1.8463180362860192, 'step': 1730}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:02,858 >> {'loss': 0.034690987, 'learning_rate': 4.8762736e-06, 'epoch': 1.856990394877268, 'step': 1740}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:05,595 >> {'loss': 0.034811094, 'learning_rate': 4.512372e-06, 'epoch': 1.8676627534685166, 'step': 1750}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:08,328 >> {'loss': 0.03468792, 'learning_rate': 4.1484714e-06, 'epoch': 1.8783351120597653, 'step': 1760}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:11,054 >> {'loss': 0.034560572, 'learning_rate': 3.78457e-06, 'epoch': 1.8890074706510138, 'step': 1770}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:13,787 >> {'loss': 0.034403093, 'learning_rate': 3.4206685e-06, 'epoch': 1.8996798292422625, 'step': 1780}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:16,513 >> {'loss': 0.034459766, 'learning_rate': 3.0567676e-06, 'epoch': 1.9103521878335112, 'step': 1790}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:19,243 >> {'loss': 0.03443023, 'learning_rate': 2.692866e-06, 'epoch': 1.9210245464247597, 'step': 1800}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:21,962 >> {'loss': 0.034388628, 'learning_rate': 2.328965e-06, 'epoch': 1.9316969050160084, 'step': 1810}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:24,685 >> {'loss': 0.034116223, 'learning_rate': 1.9650668e-06, 'epoch': 1.9423692636072571, 'step': 1820}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:27,418 >> {'loss': 0.03401441, 'learning_rate': 1.6011653e-06, 'epoch': 1.9530416221985059, 'step': 1830}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:30,144 >> {'loss': 0.033980813, 'learning_rate': 1.2372642e-06, 'epoch': 1.9637139807897546, 'step': 1840}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:32,868 >> {'loss': 0.03414419, 'learning_rate': 8.73363e-07, 'epoch': 1.9743863393810033, 'step': 1850}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:35,602 >> {'loss': 0.0341465, 'learning_rate': 5.0946176e-07, 'epoch': 1.985058697972252, 'step': 1860}\n",
            "[INFO|trainer_tf.py:404] 2021-06-18 13:23:38,334 >> {'loss': 0.034044337, 'learning_rate': 1.455605e-07, 'epoch': 1.9957310565635007, 'step': 1870}\n",
            "[INFO|trainer_tf.py:610] 2021-06-18 13:23:39,445 >> Training took: 0:09:23.939862\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxwnjFtc4jZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4dad9dc-a1d8-4a3d-c748-897053b290cd"
      },
      "source": [
        "token_model.save_pretrained('./my_tokenmodel/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:351] 2021-06-18 13:23:39,460 >> Configuration saved in ./my_tokenmodel/config.json\n",
            "[INFO|modeling_tf_utils.py:1053] 2021-06-18 13:23:40,119 >> Model weights saved in ./my_tokenmodel/tf_model.h5\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzcve0wA5A-V"
      },
      "source": [
        "#!zip -r my_model.zip ./my_tokenmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9pUdf68qHn"
      },
      "source": [
        "## Section 8 - Evaluate the Model\n",
        "\n",
        "Run the following cells below to evaluate your model performance.\n",
        "\n",
        "Obviously, you can only do this AFTER your training is completed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqobMwQOGiYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3606aa6-188b-44b5-92eb-b53d9f518886"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForTokenClassification\n",
        ")\n",
        "                          \n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:517] 2021-06-18 13:23:40,445 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:553] 2021-06-18 13:23:40,447 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.7.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-18 13:23:42,075 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-18 13:23:42,076 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-18 13:23:42,084 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-18 13:23:42,085 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-18 13:23:42,089 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cDPJ8cd_Mkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18b5746-7ce5-4a67-d44a-b7b05c9f6c3f"
      },
      "source": [
        "model = TFAutoModelForTokenClassification.from_pretrained('my_tokenmodel')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:515] 2021-06-18 13:23:42,125 >> loading configuration file my_tokenmodel/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-06-18 13:23:42,135 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.7.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_tf_utils.py:1259] 2021-06-18 13:23:42,137 >> loading weights file my_tokenmodel/tf_model.h5\n",
            "[WARNING|modeling_tf_utils.py:1311] 2021-06-18 13:23:42,637 >> Some layers from the model checkpoint at my_tokenmodel were not used when initializing TFDistilBertForTokenClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_tf_utils.py:1323] 2021-06-18 13:23:42,638 >> Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at my_tokenmodel and are newly initialized: ['dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrzfgM8bjJc4"
      },
      "source": [
        "def infer_tokens(text):\n",
        "    encodings = tokenizer([text], is_split_into_words=True, padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"tf\")\n",
        "\n",
        "    label_mapping = [0] * len(encodings.offset_mapping[0])\n",
        "    for i, offset in enumerate(encodings.offset_mapping[0]):\n",
        "        if encodings.offset_mapping[0][i][0] == 0 and encodings.offset_mapping[0][i][1] != 0:\n",
        "            label_mapping[i] = 1\n",
        "\n",
        "    encodings.pop(\"offset_mapping\")\n",
        "    #encodings = encodings.to(\"cuda\")\n",
        "\n",
        "    # Use the token classification model to predict the labels\n",
        "    # for each word.\n",
        "    #\n",
        "    output = token_model(encodings)[0]\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for i in range(output.shape[1]):\n",
        "        if label_mapping[i] == 1:\n",
        "            result.append(np.argmax(output[0][i]).item())\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmI5r-tY-4y0",
        "outputId": "ef0d5544-302b-4896-b288-2e4661f251b1"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# This function takes in a list of sentences (texts) and passes them into the\n",
        "# infer_tokens method to tokenize and predict each word's label.\n",
        "# \n",
        "# It will then convert the list of labels into their numeric index, and\n",
        "# return both actual label and predicted label to the caller.\n",
        "#\n",
        "def get_actual_pred_y(texts, labels):\n",
        "    all_actual_y = []\n",
        "    all_pred_y = []\n",
        "\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        x = texts[i]\n",
        "\n",
        "        actual_y = list(filter(lambda x: x != -100, labels[i]))\n",
        "        pred_y = infer_tokens(x)\n",
        "\n",
        "        if (len(actual_y) == len(pred_y)):\n",
        "            all_actual_y += actual_y\n",
        "            all_pred_y += pred_y\n",
        "        else:\n",
        "            print (\"Error: %d, %d, %d, %s \" % (i, len(actual_y), len(pred_y), x ))\n",
        "\n",
        "    return all_actual_y, all_pred_y\n",
        "\n",
        "# Get the actual and predicted labels for all words in all sentences\n",
        "# for both the training and the test set.\n",
        "# \n",
        "#actual_y_train, pred_y_train = get_actual_pred_y(train_texts, train_labels)\n",
        "actual_y_test, pred_y_test = get_actual_pred_y(val_texts, val_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3684/3684 [05:29<00:00, 11.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5jUinaajcsh",
        "outputId": "3fb965ea-3a24-4b10-8581-f44389041b6d"
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "print(classification_report(actual_y_test, pred_y_test, target_names=token_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.99      0.99      0.99     38554\n",
            "       B-PER       0.97      0.96      0.96      1617\n",
            "       I-PER       0.98      0.99      0.98      1156\n",
            "       B-ORG       0.90      0.88      0.89      1661\n",
            "       I-ORG       0.84      0.87      0.86       835\n",
            "       B-LOC       0.91      0.93      0.92      1668\n",
            "       I-LOC       0.80      0.92      0.86       257\n",
            "      B-MISC       0.80      0.82      0.81       702\n",
            "      I-MISC       0.61      0.70      0.65       216\n",
            "\n",
            "    accuracy                           0.98     46666\n",
            "   macro avg       0.87      0.90      0.88     46666\n",
            "weighted avg       0.98      0.98      0.98     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VabGEW_qVZ2t"
      },
      "source": [
        "Ok, let's test it on your own text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQMQ2DabQHjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a671a970-8651-4c67-e371-4de4085e0982"
      },
      "source": [
        "text = input()\n",
        "\n",
        "text = text.split(\" \")\n",
        "\n",
        "print(text)\n",
        "predicted = [token_labels[label] for label in infer_tokens(text)]\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peter parker is going to CPF to file a claim.\n",
            "['Peter', 'parker', 'is', 'going', 'to', 'CPF', 'to', 'file', 'a', 'claim.']\n",
            "['B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyCgX19HUeeq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}