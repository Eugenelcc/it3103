{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf2env)",
      "language": "python",
      "name": "tf2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "bert-embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/session-12/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKanFNbpTfv4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/master/session-8/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>\n",
        "<br/>\n",
        "\n",
        "# Using BERT as Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8phFkS4Tfv7"
      },
      "source": [
        "Other than fine-tuning BERT for downstream task such as text classification, we can use pretrained BERT model as a feature extractor, very much the same as we are using pretrained CNN such as ResNet as feature extractors for downstream task such as image classification and object detection.  \n",
        "\n",
        "In this lab, we will see how we use a pretrained DistilBert Model to extract features (or embedding) from text and use the extracted features (embeddings) to train a classifier to classify text. You can contrast this with the other lab where we train the DistilBert end to end for the classification, and compare the performance of both. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- extract text embeddings from the bert model \n",
        "- use the extracted features for text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcAO5A0oVMOj",
        "outputId": "29af1deb-347d-4e80-92d3-4ee348fc1807"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiiqcrhLTfv8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os \n",
        "import shutil\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModel,\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh7XepZEZ3Ll",
        "outputId": "be219369-65c5-4d83-c001-14ac6cdfb178"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README', 'imdbEr.txt', 'train', 'test', 'imdb.vocab']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we7dCLYWZ45L",
        "outputId": "f6a767b2-177c-4ca2-f786-50879555e286"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos',\n",
              " 'urls_unsup.txt',\n",
              " 'labeledBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'neg',\n",
              " 'urls_neg.txt',\n",
              " 'unsup']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjYgKs2SaFZQ"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz7btxVXaQ4U",
        "outputId": "8c856a93-36ec-422c-e3bb-e15fe40d08e7"
      },
      "source": [
        "batch_size = 128\n",
        "seed = 123\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='training', seed=seed)\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='validation', seed=seed)\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', batch_size=batch_size, seed=seed)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmfoRiIPbW4W"
      },
      "source": [
        "batches = list(train_ds.as_numpy_iterator())\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "for batch in batches:\n",
        "    texts, labels = batch[0], batch[1]\n",
        "    texts_labels = zip(texts, labels)\n",
        "    for text, label in texts_labels:\n",
        "        train_texts.append(str(text))\n",
        "        train_labels.append(label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp2xHjgXcsLI",
        "outputId": "29c7ac42-2c62-44a1-9612-8741ff3d45b2"
      },
      "source": [
        "len(train_labels), len(train_texts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM0LvjppeeZG"
      },
      "source": [
        "batches = list(val_ds.as_numpy_iterator())\n",
        "\n",
        "val_texts = []\n",
        "val_labels = []\n",
        "for batch in batches:\n",
        "    texts, labels = batch[0], batch[1]\n",
        "    texts_labels = zip(texts, labels)\n",
        "    for text, label in texts_labels:\n",
        "        val_texts.append(str(text))\n",
        "        val_labels.append(label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qWrMeuPgID1",
        "outputId": "ccf8a72a-8e4c-4a1f-8bf7-70cda4dbeed3"
      },
      "source": [
        "len(val_labels), len(val_texts)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7tTMMJDf7aG"
      },
      "source": [
        "batches = list(test_ds.as_numpy_iterator())\n",
        "\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for batch in batches:\n",
        "    texts, labels = batch[0], batch[1]\n",
        "    texts_labels = zip(texts, labels)\n",
        "    for text, label in texts_labels:\n",
        "        test_texts.append(str(text))\n",
        "        test_labels.append(label)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MVZSR_VgLzi",
        "outputId": "47285cb0-6a41-4b27-f737-e6794e4be8fc"
      },
      "source": [
        "len(test_labels), len(test_texts)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMPDA1KwgaXZ",
        "outputId": "3ec27925-0b49-42e0-f086-fc67b4ae02ce"
      },
      "source": [
        "type(train_texts[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Qf2HkGTfv8"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VDJrnlTfv-"
      },
      "source": [
        "We will just use a small subset of the training and test data for this lab, as the feature extraction can take a long time, even with GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URk2vl-WTfv-"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_texts = train_texts[:2000]\n",
        "train_labels = train_labels[:2000]\n",
        "test_texts = test_texts[:200]\n",
        "test_labels = test_labels[:200]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlKwCN6vmXyi"
      },
      "source": [
        "labels = np.array(train_labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4UWMFHjnbCU",
        "outputId": "4759545a-db3d-4939-fb57-969261e16e29"
      },
      "source": [
        "np.unique(test_labels, return_counts=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1], dtype=int32), array([ 98, 102]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboqVJWTfwA"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  This is the same as the other lab exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5THnkPITfwA",
        "outputId": "3e1f20dd-9b38-42c4-a76f-700e216ece94"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:517] 2021-06-16 06:14:10,486 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:553] 2021-06-16 06:14:10,488 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:14:11,981 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:14:11,982 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:14:11,987 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:14:11,988 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-06-16 06:14:11,992 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ltReujTfwA"
      },
      "source": [
        "The pretrained DistilBERT [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) expects a string or list of string, so we need to convert the data frame (or series) into list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFbiO3STfwB"
      },
      "source": [
        "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnqjMMCWTfwB"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgdY5C9TfwB"
      },
      "source": [
        "We will create a tensorflow dataset and use it's efficient batching later to obtain the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Znx8EnTfwB"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HNwF7yYTfwB"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    train_encodings['input_ids'],\n",
        "    train_labels\n",
        ")).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    test_encodings['input_ids'],\n",
        "    test_labels\n",
        ")).batch(BATCH_SIZE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU6uUMnATfwC"
      },
      "source": [
        "train_data = train_dataset.as_numpy_iterator()\n",
        "test_data = test_dataset.as_numpy_iterator()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awI02BNyhRvO",
        "outputId": "0ea89bf2-6d2a-47d1-b251-90496da24844"
      },
      "source": [
        "len(train_encodings['input_ids'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBuGjl7fTfwC"
      },
      "source": [
        "Here we instantiate a pretrained model from 'distilbert-base-cased' and specify output_hidden_state=True so that we get the output from each of the attention layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Pgl3CDTfwC"
      },
      "source": [
        "## Feature Extraction using (Distil)BERT. \n",
        "\n",
        "Here we will load the pretrained model for distibert-based-uncased and use it to extract features from the text (i.e. emeddings). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKSzCCRyTfwC",
        "outputId": "0f0688a1-280a-46bb-918e-5c9b1c99492b"
      },
      "source": [
        "model = TFAutoModel.from_pretrained(\"distilbert-base-cased\",output_hidden_states=True)\n",
        "#model = TFAutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:517] 2021-06-16 06:14:15,967 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:553] 2021-06-16 06:14:15,971 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_tf_utils.py:1261] 2021-06-16 06:14:16,266 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/fe773335fbb46b412a9093627b6c3235a69c55bad3bd1deee40813cd0a8d0a82.33c483181ffc4c7cbdd0b733245bcc9b479f14f3b2e892f635fe03f4f3a41495.h5\n",
            "[WARNING|modeling_tf_utils.py:1311] 2021-06-16 06:14:17,306 >> Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_tf_utils.py:1329] 2021-06-16 06:14:17,307 >> All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrWYnDBTfwD"
      },
      "source": [
        "The model will produce two outputs: the 1st output `output[0]` is of shape `(16, 512, 768)` which corresponds to the output of the last hidden layer and the second output `output[1]` is a list of 7 outputs of shape `(16, 512, 768)`, corresponding to the output of each of the 7 attention layers. 768 refers to the hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B793BJtwTfwD",
        "outputId": "717184a7-a919-4d50-dd74-2989f5996630"
      },
      "source": [
        "train_embeddings = None\n",
        "\n",
        "for batch in train_data:\n",
        "    output = model.predict(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if train_embeddings is None:\n",
        "        train_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        train_embeddings = np.vstack([train_embeddings, sentence_embeddings])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdd9aed4de0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdd9aed4de0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fddb6781dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fddb6781dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5i4L7XTfwD"
      },
      "source": [
        "test_embeddings = None\n",
        "\n",
        "for batch in test_data:\n",
        "    output = model.predict(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if test_embeddings is None:\n",
        "        test_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        test_embeddings = np.vstack([test_embeddings, sentence_embeddings])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhAG0WnTfwD"
      },
      "source": [
        "## Train a classifier using the extracted features (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zOWEm85TfwE"
      },
      "source": [
        "X_train = train_embeddings\n",
        "y_train = train_labels"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fzAPLL2TfwE"
      },
      "source": [
        "X_test = test_embeddings\n",
        "y_test = test_labels"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiA0JMdzTfwE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQhBq-1sTfwE",
        "outputId": "3a904074-f88e-48ec-e5f8-ce01e716eb27"
      },
      "source": [
        "clf = LinearSVC()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84        98\n",
            "           1       0.84      0.85      0.85       102\n",
            "\n",
            "    accuracy                           0.84       200\n",
            "   macro avg       0.85      0.84      0.84       200\n",
            "weighted avg       0.85      0.84      0.84       200\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyaH5WnxTfwE"
      },
      "source": [
        "We should be getting an accuracy score of around 80% which is quite good, considering we are training with only 2000 samples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVDxRUdTfwE"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "1. Modify the code to use the output from a different attention layer as input features (embeddings) to the classifier. \n",
        "2. Try to generate sentence embeddings using different strategy, e.g. take a average of the output from multiple layers instead of only a single layer.\n",
        "2. Modify the code to use BERT model and see if it performs better than the DistilBERT. For BERT Model, the output of different layers are in `output[2]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUesPy8ATfwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}